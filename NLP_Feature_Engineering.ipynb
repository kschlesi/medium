{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>NLP Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have streamlined the analysis with the NLP pipeline included in `NLP.py`. By creating an `NLModeler`, I can read in unstructured text, apply preprocessing, create a dictionary and a full corpus, and utilize tf-idf and topic modeling. The features calculated from these models will be explored, and combined with those already included in the `articles` and `sentences` tables in the `medium` database, and combined into a useful model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Goals:</b>\n",
    "<ol>\n",
    "<li>Read in `articles` and `sentences` dbs.</li>\n",
    "<li>Use an NLModeler to create chained tf-idf and lsa models.</li>\n",
    "<li>Calculate the mean tf-idf per sentence.</li>\n",
    "<li>Plot the distribution of stfidf by highlight label.</li>\n",
    "<li>Plot a scatter of stfidf versus other potential features.</li>\n",
    "<li>Calculate the similarity in LSA space of each sentence to its own article.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>1. Read in articles and sentences dbs.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to postgresql db\n",
    "username = 'kimberly'\n",
    "dbname = 'medium'\n",
    "\n",
    "dbe = create_engine('postgres://%s@localhost/%s'%(username,dbname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4649, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>popdate</th>\n",
       "      <th>url</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>highlight</th>\n",
       "      <th>nlikes</th>\n",
       "      <th>ncomments</th>\n",
       "      <th>ntags</th>\n",
       "      <th>origdb</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>npar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1015a0f4961d</th>\n",
       "      <td>Day One: President Obama and the First Family ...</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>https://medium.com/@ObamaWhiteHouse/day-one-pr...</td>\n",
       "      <td>ca9f8f16893b</td>\n",
       "      <td>The Obama White House</td>\n",
       "      <td>Today, Air Force One touched down here in Hava...</td>\n",
       "      <td>336</td>\n",
       "      <td>15</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cuba,Twitter,Cuba Trip</td>\n",
       "      <td>¡Hola desde cuba! Today, Air Force One touched...</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101a407e8c61</th>\n",
       "      <td>You don’t ‘make it’ — it makes you.</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>https://medium.com/the-mission/you-dont-make-i...</td>\n",
       "      <td>5ce28105ffbc</td>\n",
       "      <td>Jon Westenberg</td>\n",
       "      <td>You don’t ‘make it’ — it makes you.</td>\n",
       "      <td>549</td>\n",
       "      <td>37</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Entrepreneurship,Startup,Life</td>\n",
       "      <td>I always wanted to make it. I grew up dreaming...</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030d29376f1</th>\n",
       "      <td>UX: Infinite Scrolling vs. Pagination</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>https://uxplanet.org/ux-infinite-scrolling-vs-...</td>\n",
       "      <td>bcab753a4d4e</td>\n",
       "      <td>Nick Babich</td>\n",
       "      <td>There are only a few instances where infinite ...</td>\n",
       "      <td>1.91K</td>\n",
       "      <td>46</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>UX,Design,User Experience,UX Design</td>\n",
       "      <td>“Should I use Infinite scrolling or Pagination...</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315016b299</th>\n",
       "      <td>A Lesson on Stereotypes</td>\n",
       "      <td>2016-08-20</td>\n",
       "      <td>https://medium.com/@mramsburg85/a-lesson-on-st...</td>\n",
       "      <td>d38709ba4e06</td>\n",
       "      <td>Michael Ramsburg</td>\n",
       "      <td>Stereotypes strip you of your culture, like ou...</td>\n",
       "      <td>583</td>\n",
       "      <td>103</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Stereotypes,Appalachia,Culture,Essay,Opinion</td>\n",
       "      <td>Stereotypes./nMrs. Mitchell, my sixth grade te...</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321e751c6d</th>\n",
       "      <td>For This Republican, Never Trump Means “I’m Wi...</td>\n",
       "      <td>2016-07-30</td>\n",
       "      <td>https://medium.com/@ccmccain/for-this-republic...</td>\n",
       "      <td>4e965facd5f9</td>\n",
       "      <td>Caroline McCain</td>\n",
       "      <td>Trump’s statement, in my view, is unforgivable...</td>\n",
       "      <td>2.5K</td>\n",
       "      <td>302</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hillary Clinton,Donald Trump,Never Trump,2016 ...</td>\n",
       "      <td>If you know me at all, you know I am a woman f...</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          title     popdate  \\\n",
       "postid                                                                        \n",
       "1015a0f4961d  Day One: President Obama and the First Family ...  2016-03-21   \n",
       "101a407e8c61                You don’t ‘make it’ — it makes you.  2016-06-02   \n",
       "1030d29376f1              UX: Infinite Scrolling vs. Pagination  2016-05-02   \n",
       "10315016b299                            A Lesson on Stereotypes  2016-08-20   \n",
       "10321e751c6d  For This Republican, Never Trump Means “I’m Wi...  2016-07-30   \n",
       "\n",
       "                                                            url        userid  \\\n",
       "postid                                                                          \n",
       "1015a0f4961d  https://medium.com/@ObamaWhiteHouse/day-one-pr...  ca9f8f16893b   \n",
       "101a407e8c61  https://medium.com/the-mission/you-dont-make-i...  5ce28105ffbc   \n",
       "1030d29376f1  https://uxplanet.org/ux-infinite-scrolling-vs-...  bcab753a4d4e   \n",
       "10315016b299  https://medium.com/@mramsburg85/a-lesson-on-st...  d38709ba4e06   \n",
       "10321e751c6d  https://medium.com/@ccmccain/for-this-republic...  4e965facd5f9   \n",
       "\n",
       "                           username  \\\n",
       "postid                                \n",
       "1015a0f4961d  The Obama White House   \n",
       "101a407e8c61         Jon Westenberg   \n",
       "1030d29376f1            Nick Babich   \n",
       "10315016b299       Michael Ramsburg   \n",
       "10321e751c6d        Caroline McCain   \n",
       "\n",
       "                                                      highlight nlikes  \\\n",
       "postid                                                                   \n",
       "1015a0f4961d  Today, Air Force One touched down here in Hava...    336   \n",
       "101a407e8c61                You don’t ‘make it’ — it makes you.    549   \n",
       "1030d29376f1  There are only a few instances where infinite ...  1.91K   \n",
       "10315016b299  Stereotypes strip you of your culture, like ou...    583   \n",
       "10321e751c6d  Trump’s statement, in my view, is unforgivable...   2.5K   \n",
       "\n",
       "             ncomments  ntags  origdb  \\\n",
       "postid                                  \n",
       "1015a0f4961d        15    3.0     3.0   \n",
       "101a407e8c61        37    3.0     3.0   \n",
       "1030d29376f1        46    4.0     3.0   \n",
       "10315016b299       103    5.0     3.0   \n",
       "10321e751c6d       302    5.0     3.0   \n",
       "\n",
       "                                                           tags  \\\n",
       "postid                                                            \n",
       "1015a0f4961d                             Cuba,Twitter,Cuba Trip   \n",
       "101a407e8c61                      Entrepreneurship,Startup,Life   \n",
       "1030d29376f1                UX,Design,User Experience,UX Design   \n",
       "10315016b299       Stereotypes,Appalachia,Culture,Essay,Opinion   \n",
       "10321e751c6d  Hillary Clinton,Donald Trump,Never Trump,2016 ...   \n",
       "\n",
       "                                                           text  npar  \n",
       "postid                                                                 \n",
       "1015a0f4961d  ¡Hola desde cuba! Today, Air Force One touched...  20.0  \n",
       "101a407e8c61  I always wanted to make it. I grew up dreaming...  21.0  \n",
       "1030d29376f1  “Should I use Infinite scrolling or Pagination...  34.0  \n",
       "10315016b299  Stereotypes./nMrs. Mitchell, my sixth grade te...  12.0  \n",
       "10321e751c6d  If you know me at all, you know I am a woman f...  45.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get articles df, drop missing data\n",
    "dfA = pd.read_sql('articles', dbe, index_col='postid')\n",
    "dfA = dfA.dropna(axis=0,how='any')\n",
    "print(dfA.shape)\n",
    "dfA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434792, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alength</th>\n",
       "      <th>index</th>\n",
       "      <th>postid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>slabel</th>\n",
       "      <th>sposition</th>\n",
       "      <th>swcount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{hola,desde,cuba}</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{today,air,force,one,touched,havana,first,time...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{question,remarkable,moment,relationship,unite...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{also,landmark,progress,made,since,president,o...</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{trip,also,professionally,personally,meaningfu...</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alength  index        postid  \\\n",
       "level_0                                 \n",
       "0             52    0.0  1015a0f4961d   \n",
       "1             52    0.0  1015a0f4961d   \n",
       "2             52    0.0  1015a0f4961d   \n",
       "3             52    0.0  1015a0f4961d   \n",
       "4             52    0.0  1015a0f4961d   \n",
       "\n",
       "                                                  sentence  slabel  sposition  \\\n",
       "level_0                                                                         \n",
       "0                                        {hola,desde,cuba}   False          0   \n",
       "1        {today,air,force,one,touched,havana,first,time...    True          1   \n",
       "2        {question,remarkable,moment,relationship,unite...   False          2   \n",
       "3        {also,landmark,progress,made,since,president,o...   False          3   \n",
       "4        {trip,also,professionally,personally,meaningfu...   False          4   \n",
       "\n",
       "         swcount  \n",
       "level_0           \n",
       "0              3  \n",
       "1              9  \n",
       "2              9  \n",
       "3             29  \n",
       "4             19  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get sentences df\n",
    "dfS = pd.read_sql('sentences', dbe, index_col='level_0')\n",
    "print(dfS.shape)\n",
    "dfS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2. Use an NLModeler to create chained tf-idf and lsa models.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NLP.NLModeler object at 0x115f2e7b8>\n"
     ]
    }
   ],
   "source": [
    "mText = NLP.NLModeler(list(dfA.text))\n",
    "print(mText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4640, 741)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>731</th>\n",
       "      <th>732</th>\n",
       "      <th>733</th>\n",
       "      <th>734</th>\n",
       "      <th>735</th>\n",
       "      <th>736</th>\n",
       "      <th>737</th>\n",
       "      <th>738</th>\n",
       "      <th>739</th>\n",
       "      <th>740</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[hola, desd, cuba]</td>\n",
       "      <td>[today, air, forc, one, touch, havana, first, ...</td>\n",
       "      <td>[question, remark, moment, relationship, unit,...</td>\n",
       "      <td>[also, landmark, progress, made, sinc, presid,...</td>\n",
       "      <td>[trip, also, profession, person, meaning, spec...</td>\n",
       "      <td>[good, illustr, close, two, countri, link, tri...</td>\n",
       "      <td>[look, forward, learn, first, hand, cuban, cul...</td>\n",
       "      <td>[daili, basi, answer, question, presid, trip, ...</td>\n",
       "      <td>[pose, question, twitter, use, askpresssec, an...</td>\n",
       "      <td>[let, start, question, know, mani, american, c...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[alway, want, make]</td>\n",
       "      <td>[grew, dream, rockstar, paradigm, made, tv, se...</td>\n",
       "      <td>[paradigm, point, life, work, paid, happi]</td>\n",
       "      <td>[everyth, chang, better]</td>\n",
       "      <td>[know, idea, head, around, would, look, like, ...</td>\n",
       "      <td>[sign, record, deal, thought, would, moment, p...</td>\n",
       "      <td>[know, felt, like, happen]</td>\n",
       "      <td>[differ]</td>\n",
       "      <td>[feel, differ, moment, chang, life, big, way]</td>\n",
       "      <td>[year, later, fell, apart, drop, label, chang,...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[use, infinit, scroll, pagin, content]</td>\n",
       "      <td>[design, still, refere, tug, war, two, method,...</td>\n",
       "      <td>[strength, weak, articl, overview, two, method...</td>\n",
       "      <td>[infinit, scroll, techniqu, allow, user, scrol...</td>\n",
       "      <td>[techniqu, simpli, keep, refresh, page, scroll]</td>\n",
       "      <td>[tempt, may, sound, techniqu, one, size, fit, ...</td>\n",
       "      <td>[use, scroll, prime, method, explor, data, may...</td>\n",
       "      <td>[popular, social, media, massiv, amount, data,...</td>\n",
       "      <td>[infinit, scroll, almost, must, featur, discov...</td>\n",
       "      <td>[user, search, someth, specif, need, see, larg...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[stereotyp]</td>\n",
       "      <td>[mr]</td>\n",
       "      <td>[mitchel, sixth, grade, teacher, rural, appala...</td>\n",
       "      <td>[scrawl, term, big, tall, letter, stare, us, s...</td>\n",
       "      <td>[studi, moment, silent, eye, scan, face]</td>\n",
       "      <td>[voic, break, silenc, word, project, mouth, ear]</td>\n",
       "      <td>[stereotyp, say]</td>\n",
       "      <td>[take, tidbit, truth, twist, stori, fictiti, n...</td>\n",
       "      <td>[like, one, us, hillbilli]</td>\n",
       "      <td>[someon, say]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[know, know, woman, fierc, loyal, friend, famili]</td>\n",
       "      <td>[juli, 18, alreadi, knew, enough, reason, neve...</td>\n",
       "      <td>[follow, line, right, wing, wacko, bird, trump...</td>\n",
       "      <td>[insult, grandfath, attack, qualiti, loyalti, ...</td>\n",
       "      <td>[mock, sacrific, mani, given, anguish, famili,...</td>\n",
       "      <td>[grandfath, respond, grace, forgiv, man, held,...</td>\n",
       "      <td>[nurs, grudg, ever, sinc]</td>\n",
       "      <td>[trump, statement, view, unforgiv, speak, kind...</td>\n",
       "      <td>[take, long, move, beyond, person, week, week,...</td>\n",
       "      <td>[lack, tempera, wisdom, navig, ever, increasin...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 741 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 0    \\\n",
       "0                                 [hola, desd, cuba]   \n",
       "1                                [alway, want, make]   \n",
       "2             [use, infinit, scroll, pagin, content]   \n",
       "3                                        [stereotyp]   \n",
       "4  [know, know, woman, fierc, loyal, friend, famili]   \n",
       "\n",
       "                                                 1    \\\n",
       "0  [today, air, forc, one, touch, havana, first, ...   \n",
       "1  [grew, dream, rockstar, paradigm, made, tv, se...   \n",
       "2  [design, still, refere, tug, war, two, method,...   \n",
       "3                                               [mr]   \n",
       "4  [juli, 18, alreadi, knew, enough, reason, neve...   \n",
       "\n",
       "                                                 2    \\\n",
       "0  [question, remark, moment, relationship, unit,...   \n",
       "1         [paradigm, point, life, work, paid, happi]   \n",
       "2  [strength, weak, articl, overview, two, method...   \n",
       "3  [mitchel, sixth, grade, teacher, rural, appala...   \n",
       "4  [follow, line, right, wing, wacko, bird, trump...   \n",
       "\n",
       "                                                 3    \\\n",
       "0  [also, landmark, progress, made, sinc, presid,...   \n",
       "1                           [everyth, chang, better]   \n",
       "2  [infinit, scroll, techniqu, allow, user, scrol...   \n",
       "3  [scrawl, term, big, tall, letter, stare, us, s...   \n",
       "4  [insult, grandfath, attack, qualiti, loyalti, ...   \n",
       "\n",
       "                                                 4    \\\n",
       "0  [trip, also, profession, person, meaning, spec...   \n",
       "1  [know, idea, head, around, would, look, like, ...   \n",
       "2    [techniqu, simpli, keep, refresh, page, scroll]   \n",
       "3           [studi, moment, silent, eye, scan, face]   \n",
       "4  [mock, sacrific, mani, given, anguish, famili,...   \n",
       "\n",
       "                                                 5    \\\n",
       "0  [good, illustr, close, two, countri, link, tri...   \n",
       "1  [sign, record, deal, thought, would, moment, p...   \n",
       "2  [tempt, may, sound, techniqu, one, size, fit, ...   \n",
       "3   [voic, break, silenc, word, project, mouth, ear]   \n",
       "4  [grandfath, respond, grace, forgiv, man, held,...   \n",
       "\n",
       "                                                 6    \\\n",
       "0  [look, forward, learn, first, hand, cuban, cul...   \n",
       "1                         [know, felt, like, happen]   \n",
       "2  [use, scroll, prime, method, explor, data, may...   \n",
       "3                                   [stereotyp, say]   \n",
       "4                          [nurs, grudg, ever, sinc]   \n",
       "\n",
       "                                                 7    \\\n",
       "0  [daili, basi, answer, question, presid, trip, ...   \n",
       "1                                           [differ]   \n",
       "2  [popular, social, media, massiv, amount, data,...   \n",
       "3  [take, tidbit, truth, twist, stori, fictiti, n...   \n",
       "4  [trump, statement, view, unforgiv, speak, kind...   \n",
       "\n",
       "                                                 8    \\\n",
       "0  [pose, question, twitter, use, askpresssec, an...   \n",
       "1      [feel, differ, moment, chang, life, big, way]   \n",
       "2  [infinit, scroll, almost, must, featur, discov...   \n",
       "3                         [like, one, us, hillbilli]   \n",
       "4  [take, long, move, beyond, person, week, week,...   \n",
       "\n",
       "                                                 9    ...    731   732   733  \\\n",
       "0  [let, start, question, know, mani, american, c...  ...   None  None  None   \n",
       "1  [year, later, fell, apart, drop, label, chang,...  ...   None  None  None   \n",
       "2  [user, search, someth, specif, need, see, larg...  ...   None  None  None   \n",
       "3                                      [someon, say]  ...   None  None  None   \n",
       "4  [lack, tempera, wisdom, navig, ever, increasin...  ...   None  None  None   \n",
       "\n",
       "    734   735   736   737   738   739   740  \n",
       "0  None  None  None  None  None  None  None  \n",
       "1  None  None  None  None  None  None  None  \n",
       "2  None  None  None  None  None  None  None  \n",
       "3  None  None  None  None  None  None  None  \n",
       "4  None  None  None  None  None  None  None  \n",
       "\n",
       "[5 rows x 741 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mText.process_text(break_on=['.'], init_split_on='database', origdb=list(dfA.origdb))\n",
    "dfP = mText.get_text(ttype='tokenized',output_type='dataframe')\n",
    "print(dfP.shape)\n",
    "dfP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mText.make_dictionary()\n",
    "mText.load_corpus('alldoc_corpus.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mText.make_tfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = mText.tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'atext' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cbad0194c6a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbreak_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tokenized'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Insight/Medium/NLP.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, in_text, break_on, stopwords, to_stem, init_split_on, origdb, ddiv_max_length)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0matext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprocess_paragraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mswords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_stem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessed_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0matext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'atext' referenced before assignment"
     ]
    }
   ],
   "source": [
    "new_vec = NLP.NLProcessor(['new sentence'])\n",
    "# make this more general....\n",
    "new_vec.process_text(break_on=['.'])\n",
    "new_vec.get_text(ttype='tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mText.dictionary.doc2bow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
