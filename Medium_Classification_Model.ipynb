{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and Testing a Classification Model</h3>\n",
    "\n",
    "In this notebook, I will build a training/test/validation set of sentences from Medium articles. I will label the set and extract features. Then I will train a model and cross-validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step One:</b> Separate out training from test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to postgresql db\n",
    "username = 'kimberly'\n",
    "dbname = 'medium'\n",
    "\n",
    "dbe = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get df, drop missing data\n",
    "df = pd.read_sql('articles', dbe, index_col='postid')\n",
    "df = df.dropna(axis=0,how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Functions to format row of the df, as well as do text processing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to convert nlikes and ncomments to integer\n",
    "def convert_K(nstr):\n",
    "    spl = nstr.split('K')\n",
    "    if len(spl)==1:\n",
    "        return int(float(spl[0]))\n",
    "    else:\n",
    "        return int(float(spl[0])*1000)\n",
    "    \n",
    "def convert_str(nstr):\n",
    "    nstr = nstr.replace(',','')\n",
    "    if nstr=='':\n",
    "        return None\n",
    "    else:\n",
    "        return int(nstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_paragraph(par,swords):\n",
    "    '''takes one paragraph (string); performs lower, tokenize, remove punctuation/stop words'''\n",
    "    par = par.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(par)\n",
    "    nstop_tokens = [t for t in tokens if (t not in swords and t not in string.punctuation)]\n",
    "    return nstop_tokens                      \n",
    "\n",
    "def process_text_paragraphs(atext,origdb,swords):\n",
    "    '''atext is a list or series of textstrings (one from each article), \n",
    "    origdb is a list or series of corresponding original databse IDs (ints from 0-4)'''\n",
    "    # initial text split\n",
    "    alist = [initial_text_split(a,int(o)) for a,o in zip(atext,origdb)]\n",
    "        \n",
    "    # remove very long articles\n",
    "    removed_articles = [aix for aix,a in enumerate(alist) if len(a)>=250]\n",
    "    alist = [a for a in alist if len(a)<250]\n",
    "    \n",
    "    # process each paragraph\n",
    "    alist = [[process_paragraph(p,swords) for p in a] for a in alist]\n",
    "    \n",
    "    return [alist,removed_articles]\n",
    "\n",
    "def process_text_sentences(atext,origdb,swords):\n",
    "    '''same processing, but does a sentence breakup rather than paragraph'''\n",
    "    # initial text split\n",
    "    alist = [initial_text_split(a,int(o)) for a,o in zip(atext,origdb)]\n",
    "    \n",
    "    # remove very long articles\n",
    "    removed_articles = [aix for aix,a in enumerate(alist) if len(a)>=250]\n",
    "    alist = [a for a in alist if len(a)<250]\n",
    "    \n",
    "    # change paragraph splits to sentence splits\n",
    "    alist = [plist_to_slist(plist) for plist in alist]\n",
    "    \n",
    "    # process each paragraph\n",
    "    alist = [[process_paragraph(p,swords) for p in a] for a in alist]\n",
    "    \n",
    "    return [alist,removed_articles]\n",
    "\n",
    "def initial_text_split(article_text,origdb):\n",
    "    '''takes article text and original db and performs appropriate splitting'''\n",
    "    if origdb in [1,2,3]:\n",
    "        # split into paragraphs\n",
    "        plist = article_text.split('/n')\n",
    "\n",
    "        # remove \\n symbols from within words\n",
    "        plist = [p.replace('\\n','') for p in plist]\n",
    "    \n",
    "    else:\n",
    "        # split into paragraphs\n",
    "        plist = article_text.split('\\n')\n",
    "        \n",
    "    return plist\n",
    "\n",
    "def plist_to_slist(plist):\n",
    "    '''changes a list of paragraphs to a list of sentences'''\n",
    "    spl = [re.split('[/./!/?]',par) for par in plist]\n",
    "    return [s for p in spl for s in p if len(s)>1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hola', 'desde', 'cuba'], ['today', 'air', 'force', 'one', 'touched', 'havana', 'first', 'time', 'history'], ['question', 'remarkable', 'moment', 'relationship', 'united', 'states', 'cuba', 'governments', 'people'], ['also', 'landmark', 'progress', 'made', 'since', 'president', 'obama', 'decided', 'reform', 'failed', 'cold', 'war', 'era', 'policies', 'past', 'chart', 'new', 'course', 'would', 'actually', 'advance', 'american', 'interests', 'values', 'help', 'cuban', 'people', 'improve', 'lives'], ['trip', 'also', 'professionally', 'personally', 'meaningful', 'special', 'assistant', 'advisor', 'antoinette', 'rangel', 'cuban', 'american', 'learned', 'country', 'stories', 'abuela', 'maria', 'shared', 'growing'], ['good', 'illustration', 'closely', 'two', 'countries', 'linked', 'trip', 'potential', 'change', 'lives', 'families', 'cuba', 'united', 'states'], ['looking', 'forward', 'learning', 'first', 'hand', 'cuban', 'culture', 'life', 'bringing', 'white', 'house', 'press', 'corps', 'along', 'ride'], ['daily', 'basis', 'answering', 'questions', 'president', 'trip', 'cuba'], ['pose', 'question', 'twitter', 'using', 'askpresssec', 'answer', 'presssec', 'ground'], ['let', 'start', 'questions', 'know', 'many', 'americans', 'cuba', 'visit'], ['q', 'president', 'decide', 'change', 'u'], ['policy', 'toward', 'cuba'], [], ['fifty', 'years', 'united', 'states', 'wedded', 'policy', 'isolate', 'pressure', 'cuba', 'without', 'seeing', 'results'], ['cuba', 'political', 'system', 'change', 'making', 'life', 'better', 'cuban', 'people'], ['citing', 'lack', 'progress', 'december', '17', '2014', 'president', 'announced', 'country', 'policy', 'toward', 'cuba', 'changing', 'course'], ['since', 'made', 'substantial', 'progress', 'normalizing', 'relations', 'pleased', 'see', 'cuban', 'people', 'overwhelmingly', 'support', 'new', 'policy'], ['fact', 'democrats', 'republicans', 'congress', 'joining', 'president', 'trip', 'good', 'illustration', 'strong', 'bipartisan', 'support', 'new', 'opening', 'cuba'], ['opened', 'embassy', 'havana', '50', 'years', 'shuttered', 'doors'], ['expanded', 'commercial', 'ties', 'made', 'easier', 'americans', 'travel', 'business'], ['restored', 'direct', 'flights', 'direct', 'mail'], ['case', 'missed', 'check', 'president', 'obama', 'letter', '76', 'year', 'old', 'woman', 'cuba', 'carried', 'first', 'mail', 'flight', '50', 'years'], ['significant', 'changes', 'welcome', 'start', 'important', 'mission', 'expanding', 'people', 'people', 'interaction', 'commercial', 'enterprise'], ['today', 'americans', 'visiting', 'cuba', 'time', 'last', '50', 'years', 'cuban', 'american', 'families', 'american', 'students', 'volunteers', 'faith', 'leaders', 'entrepreneurs'], ['president', 'trip', 'big', 'opportunity', 'advance', 'progress'], ['beginning', 'new', 'phase', 'progress', 'reflects', 'interests', 'values'], ['better', 'future', 'cuban', 'people', 'take', 'time'], ['path', 'president', 'decided', 'time', 'take', 'necessary', 'steps', 'toward', 'better', 'future', 'citizens', 'countries'], ['q', 'cuba'], ['short', 'trip', 'lot', 'get', 'done'], ['colleague', 'ben', 'rhodes', 'president', 'deputy', 'national', 'security', 'advisor', 'along', 'help', 'senior', 'advisor', 'bernadette', 'meehan', 'taking', 'lead', 'implementing', 'policy', 'negotiating', 'cuban', 'government'], ['helpfully', 'laid', 'marquee', 'events', 'president', 'scheduled'], ['highlights'], ['sunday', 'taking', 'walking', 'tour', 'old', 'havana', 'met', 'carinal', 'ortega', 'latin', 'rite', 'archbishop', 'archdiocese', 'havana', 'cardinal', 'catholic', 'church', 'tour', 'places', 'illustrate', 'history', 'cultural', 'significance', 'beautiful', 'city'], ['monday', 'lay', 'wreath', 'jose', 'marti', 'memorial', 'participate', 'discussion', 'entrepreneurship', 'opportunity', 'cuentapropistas', 'cuban', 'entrepreneurs'], ['also', 'head', 'revolutionary', 'palace', 'meet', 'president', 'ra√∫l', 'castro', 'discuss', 'together', 'make', 'easier', 'trade', 'easier', 'cubans', 'access', 'internet', 'start', 'businesses'], ['tuesday', 'first', 'family', 'attend', 'baseball', 'game', 'tampa', 'bay', 'rays', 'cuban', 'national', 'team', 'latinoamerican', 'stadium'], ['first', 'time', 'mlb', 'team', 'played', 'cuba', 'since', '1999', 'tampa', 'bay', 'rays', 'lottery', 'pick', 'among', 'teams', 'wanted', 'come', 'play'], ['one', 'things', 'sure', 'royals', 'come', 'next', 'coming'], ['q', 'president', 'hope', 'see', 'happen', 'cuba', 'visit'], ['president', 'change', 'policy', 'historic', 'visit', 'come', 'belief', 'u'], ['help', 'make', 'difference', 'cuban', 'people'], ['changing', 'way', 'americans', 'engage', 'cubans', 'foster', 'hope', 'future', 'making'], ['president', 'policies', 'geared', 'toward', 'providing', 'opportunity', 'cuban', 'people', 'rather', 'isolating', 'past'], ['course', 'real', 'differences', 'political', 'economic', 'systems'], ['difficult', 'histories', 'prevented', 'progress'], ['historic', 'trip', 'president', 'hopes', 'share', 'vision', 'americans', 'cubans', 'together', 'ensure', 'future', 'cuba', 'reflects', 'freedom', 'opportunity', 'people'], ['good', 'cuba'], ['good', 'america', 'especially', 'millions', 'americans', 'deep', 'enduring', 'ties', 'cuba'], ['follow', 'along', 'follow', 'presssec', 'twitter', 'explore', 'cuba', 'future', 'build', 'together'], ['trip', 'remember'], ['account', 'maintained', 'national', 'archives', 'records', 'administration', 'nara', 'serve', 'archive', 'obama', 'administration', 'content']]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we will use these functions to format the text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stop word corpus and process text\n",
    "swords = stopwords.words('english')\n",
    "processing_output = process_text_sentences(df.text,df.origdb,swords)\n",
    "ptext = processing_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop too-long articles\n",
    "removed_articles = processing_output[1]\n",
    "dfDrop = df.drop(df.index[removed_articles])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process highlights and titles\n",
    "htext = [plist_to_slist([hilite]) for hilite in dfDrop.highlight]\n",
    "htext = [[process_paragraph(hsent,swords) for hsent in art] for art in htext]\n",
    "ttext = [plist_to_slist([title]) for title in dfDrop.title]\n",
    "ttext = [[process_paragraph(tsent,swords) for tsent in art] for art in ttext]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ux', 'infinite', 'scrolling', 'vs'], ['pagination']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LABEL whether each sentence is in the highlight\n",
    "\n",
    "for art in htext:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
