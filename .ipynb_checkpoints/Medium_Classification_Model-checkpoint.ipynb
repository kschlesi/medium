{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and Testing a Classification Model</h3>\n",
    "\n",
    "In this notebook, I will build a training/test/validation set of sentences from Medium articles. I will label the set and extract features. Then I will train a model and cross-validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from random import randint\n",
    "from sklearn import linear_model, metrics\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step One:</b> Separate out training from test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to postgresql db\n",
    "username = 'kimberly'\n",
    "dbname = 'medium'\n",
    "\n",
    "dbe = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4649, 13)\n"
     ]
    }
   ],
   "source": [
    "# get df, drop missing data\n",
    "df = pd.read_sql('articles', dbe, index_col='postid')\n",
    "df = df.dropna(axis=0,how='any')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Functions to format row of the df, as well as do text processing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to convert nlikes and ncomments to integer\n",
    "def convert_K(nstr):\n",
    "    spl = nstr.split('K')\n",
    "    if len(spl)==1:\n",
    "        return int(float(spl[0]))\n",
    "    else:\n",
    "        return int(float(spl[0])*1000)\n",
    "    \n",
    "def convert_str(nstr):\n",
    "    nstr = nstr.replace(',','')\n",
    "    if nstr=='':\n",
    "        return None\n",
    "    else:\n",
    "        return int(nstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_paragraph(par,swords):\n",
    "    '''takes one paragraph (bare string); performs lower, tokenize, remove punctuation/stop words'''\n",
    "    par = par.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(par)\n",
    "    nstop_tokens = [t for t in tokens if (t not in swords and t not in string.punctuation)]\n",
    "    return nstop_tokens                      \n",
    "\n",
    "def process_text_paragraphs(atext,origdb,swords):\n",
    "    '''atext is a list or series of textstrings (one from each article), \n",
    "    origdb is a list or series of corresponding original databse IDs (ints from 0-4)'''\n",
    "    # initial text split\n",
    "    alist = [initial_text_split(a,int(o)) for a,o in zip(atext,origdb)]\n",
    "        \n",
    "    # remove very long articles\n",
    "    removed_articles = [aix for aix,a in enumerate(alist) if len(a)>=250]\n",
    "    alist = [a for a in alist if len(a)<250]\n",
    "    \n",
    "    # process each paragraph\n",
    "    alist = [[process_paragraph(p,swords) for p in a] for a in alist]\n",
    "    \n",
    "    return [alist,removed_articles]\n",
    "\n",
    "def process_text_sentences(atext,origdb,swords):\n",
    "    '''same processing, but does a sentence breakup rather than paragraph'''\n",
    "    # initial text split\n",
    "    alist = [initial_text_split(a,int(o)) for a,o in zip(atext,origdb)]\n",
    "    \n",
    "    # remove very long articles\n",
    "    removed_articles = [aix for aix,a in enumerate(alist) if len(a)>=250]\n",
    "    alist = [a for a in alist if len(a)<250]\n",
    "    \n",
    "    # change paragraph splits to sentence splits\n",
    "    alist = [plist_to_slist(plist) for plist in alist]\n",
    "    \n",
    "    # process each paragraph\n",
    "    alist = [[process_paragraph(p,swords) for p in a] for a in alist]\n",
    "    \n",
    "    return [alist,removed_articles]\n",
    "\n",
    "def initial_text_split(article_text,origdb):\n",
    "    '''takes article text and original db and performs appropriate splitting'''\n",
    "    if origdb in [1,2,3]:\n",
    "        # split into paragraphs\n",
    "        plist = article_text.split('/n')\n",
    "\n",
    "        # remove \\n symbols from within words\n",
    "        plist = [p.replace('\\n','') for p in plist]\n",
    "    \n",
    "    else:\n",
    "        # split into paragraphs\n",
    "        plist = article_text.split('\\n')\n",
    "        \n",
    "    return plist\n",
    "\n",
    "def plist_to_slist(plist):\n",
    "    '''changes a list of paragraphs to a list of sentences'''\n",
    "    spl = [re.split('[/./!/?]',par) for par in plist]\n",
    "    return [s for p in spl for s in p if len(s)>1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we will use these functions to format the text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n"
     ]
    }
   ],
   "source": [
    "# define stop word corpus and process text\n",
    "swords = stopwords.words('english')\n",
    "processing_output = process_text_sentences(df.text,df.origdb,swords)\n",
    "ptext = processing_output[0]\n",
    "print(len(ptext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n",
      "9\n",
      "4640\n"
     ]
    }
   ],
   "source": [
    "# drop too-long articles\n",
    "removed_articles = processing_output[1]\n",
    "dfDrop = df.drop(df.index[removed_articles])\n",
    "print(dfDrop.shape[0])\n",
    "print(len(removed_articles))\n",
    "#for rem in removed_articles:\n",
    "#    del ptext[rem]\n",
    "print(len(ptext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process highlights and titles\n",
    "htext = [plist_to_slist([hilite]) for hilite in dfDrop.highlight]\n",
    "htext = [[process_paragraph(hsent,swords) for hsent in art] for art in htext]\n",
    "ttext = [plist_to_slist([title]) for title in dfDrop.title]\n",
    "ttext = [[process_paragraph(tsent,swords) for tsent in art] for art in ttext]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n",
      "4640\n"
     ]
    }
   ],
   "source": [
    "print(dfDrop.shape[0])\n",
    "print(len(ptext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we will LABEL each sentence as 1 or 0 (in highlight or not)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4640\n"
     ]
    }
   ],
   "source": [
    "# LABEL whether each sentence is in the highlight\n",
    "plabel = []\n",
    "for a,art in enumerate(ptext):\n",
    "    alabel = []\n",
    "    for s in art:\n",
    "        alabel.append(any([(s==hs) for hs in htext[a]]))\n",
    "    plabel.append(alabel)\n",
    "print(len(plabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457\n",
      "3183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4640"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find articles with no highlight in ptext\n",
    "len(plabel)\n",
    "h_in_ptext = [any(a) for a in plabel]\n",
    "print(len(plabel) - sum(h_in_ptext))\n",
    "print(sum(h_in_ptext))\n",
    "len(plabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that several (about 1/4) of the articles with a highlight do NOT contain the highlight in the p-text (i.e., in the text scraped from p tags in the html).\n",
    "<br><br>\n",
    "These can still be used as negative examples. We will check to see if there is a corresponding positive example, and if not, use the highlight itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get the dataframe together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format rows in dataframe\n",
    "dfDrop.nlikes = dfDrop.nlikes.map(convert_K)\n",
    "dfDrop.ncomments = dfDrop.ncomments.map(convert_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2729\n"
     ]
    }
   ],
   "source": [
    "#print(len(dfDrop.iloc[637].text))\n",
    "#postidToIgnore = dfDrop.index[637]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add processed text to df\n",
    "dfDrop.text = ptext\n",
    "dfDrop.highlight = htext\n",
    "dfDrop.title = ttext\n",
    "dfDrop['label'] = plabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add article wcount to df\n",
    "wcount = [sum([len(par) for par in art]) for art in ptext]\n",
    "dfDrop['wcount'] = wcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data will include (about) 80% of the articles in the dataframe. \n",
    "First, we separate out a random set of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose test/train sets\n",
    "# test_ix = randint(0,999)\n",
    "dfTrain = dfDrop.drop(dfDrop.index[637])\n",
    "# dfTest = dfDrop.iloc([test_ix])\n",
    "# dfTrain = dfDrop.drop(dfDrop.index[test_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set up training data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>popdate</th>\n",
       "      <th>url</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>highlight</th>\n",
       "      <th>nlikes</th>\n",
       "      <th>ncomments</th>\n",
       "      <th>ntags</th>\n",
       "      <th>origdb</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>npar</th>\n",
       "      <th>label</th>\n",
       "      <th>wcount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1015a0f4961d</th>\n",
       "      <td>[[day, one, president, obama, first, family, l...</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>https://medium.com/@ObamaWhiteHouse/day-one-pr...</td>\n",
       "      <td>ca9f8f16893b</td>\n",
       "      <td>The Obama White House</td>\n",
       "      <td>[[today, air, force, one, touched, havana, fir...</td>\n",
       "      <td>336</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cuba,Twitter,Cuba Trip</td>\n",
       "      <td>[[hola, desde, cuba], [today, air, force, one,...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[False, True, False, False, False, False, Fals...</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101a407e8c61</th>\n",
       "      <td>[[make, makes]]</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>https://medium.com/the-mission/you-dont-make-i...</td>\n",
       "      <td>5ce28105ffbc</td>\n",
       "      <td>Jon Westenberg</td>\n",
       "      <td>[[make, makes]]</td>\n",
       "      <td>549</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Entrepreneurship,Startup,Life</td>\n",
       "      <td>[[always, wanted, make], [grew, dreaming, rock...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030d29376f1</th>\n",
       "      <td>[[ux, infinite, scrolling, vs], [pagination]]</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>https://uxplanet.org/ux-infinite-scrolling-vs-...</td>\n",
       "      <td>bcab753a4d4e</td>\n",
       "      <td>Nick Babich</td>\n",
       "      <td>[[instances, infinite, scrolling, effective], ...</td>\n",
       "      <td>1910</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>UX,Design,User Experience,UX Design</td>\n",
       "      <td>[[use, infinite, scrolling, pagination, conten...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315016b299</th>\n",
       "      <td>[[lesson, stereotypes]]</td>\n",
       "      <td>2016-08-20</td>\n",
       "      <td>https://medium.com/@mramsburg85/a-lesson-on-st...</td>\n",
       "      <td>d38709ba4e06</td>\n",
       "      <td>Michael Ramsburg</td>\n",
       "      <td>[[stereotypes, strip, culture, like, mountains...</td>\n",
       "      <td>583</td>\n",
       "      <td>103.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Stereotypes,Appalachia,Culture,Essay,Opinion</td>\n",
       "      <td>[[stereotypes], [mrs], [mitchell, sixth, grade...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321e751c6d</th>\n",
       "      <td>[[republican, never, trump, means]]</td>\n",
       "      <td>2016-07-30</td>\n",
       "      <td>https://medium.com/@ccmccain/for-this-republic...</td>\n",
       "      <td>4e965facd5f9</td>\n",
       "      <td>Caroline McCain</td>\n",
       "      <td>[[trump, statement, view, unforgivable, speaks...</td>\n",
       "      <td>2500</td>\n",
       "      <td>302.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hillary Clinton,Donald Trump,Never Trump,2016 ...</td>\n",
       "      <td>[[know, know, woman, fiercely, loyal, friends,...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          title     popdate  \\\n",
       "postid                                                                        \n",
       "1015a0f4961d  [[day, one, president, obama, first, family, l...  2016-03-21   \n",
       "101a407e8c61                                    [[make, makes]]  2016-06-02   \n",
       "1030d29376f1      [[ux, infinite, scrolling, vs], [pagination]]  2016-05-02   \n",
       "10315016b299                            [[lesson, stereotypes]]  2016-08-20   \n",
       "10321e751c6d                [[republican, never, trump, means]]  2016-07-30   \n",
       "\n",
       "                                                            url        userid  \\\n",
       "postid                                                                          \n",
       "1015a0f4961d  https://medium.com/@ObamaWhiteHouse/day-one-pr...  ca9f8f16893b   \n",
       "101a407e8c61  https://medium.com/the-mission/you-dont-make-i...  5ce28105ffbc   \n",
       "1030d29376f1  https://uxplanet.org/ux-infinite-scrolling-vs-...  bcab753a4d4e   \n",
       "10315016b299  https://medium.com/@mramsburg85/a-lesson-on-st...  d38709ba4e06   \n",
       "10321e751c6d  https://medium.com/@ccmccain/for-this-republic...  4e965facd5f9   \n",
       "\n",
       "                           username  \\\n",
       "postid                                \n",
       "1015a0f4961d  The Obama White House   \n",
       "101a407e8c61         Jon Westenberg   \n",
       "1030d29376f1            Nick Babich   \n",
       "10315016b299       Michael Ramsburg   \n",
       "10321e751c6d        Caroline McCain   \n",
       "\n",
       "                                                      highlight  nlikes  \\\n",
       "postid                                                                    \n",
       "1015a0f4961d  [[today, air, force, one, touched, havana, fir...     336   \n",
       "101a407e8c61                                    [[make, makes]]     549   \n",
       "1030d29376f1  [[instances, infinite, scrolling, effective], ...    1910   \n",
       "10315016b299  [[stereotypes, strip, culture, like, mountains...     583   \n",
       "10321e751c6d  [[trump, statement, view, unforgivable, speaks...    2500   \n",
       "\n",
       "              ncomments  ntags  origdb  \\\n",
       "postid                                   \n",
       "1015a0f4961d       15.0    3.0     3.0   \n",
       "101a407e8c61       37.0    3.0     3.0   \n",
       "1030d29376f1       46.0    4.0     3.0   \n",
       "10315016b299      103.0    5.0     3.0   \n",
       "10321e751c6d      302.0    5.0     3.0   \n",
       "\n",
       "                                                           tags  \\\n",
       "postid                                                            \n",
       "1015a0f4961d                             Cuba,Twitter,Cuba Trip   \n",
       "101a407e8c61                      Entrepreneurship,Startup,Life   \n",
       "1030d29376f1                UX,Design,User Experience,UX Design   \n",
       "10315016b299       Stereotypes,Appalachia,Culture,Essay,Opinion   \n",
       "10321e751c6d  Hillary Clinton,Donald Trump,Never Trump,2016 ...   \n",
       "\n",
       "                                                           text  npar  \\\n",
       "postid                                                                  \n",
       "1015a0f4961d  [[hola, desde, cuba], [today, air, force, one,...  20.0   \n",
       "101a407e8c61  [[always, wanted, make], [grew, dreaming, rock...  21.0   \n",
       "1030d29376f1  [[use, infinite, scrolling, pagination, conten...  34.0   \n",
       "10315016b299  [[stereotypes], [mrs], [mitchell, sixth, grade...  12.0   \n",
       "10321e751c6d  [[know, know, woman, fiercely, loyal, friends,...  45.0   \n",
       "\n",
       "                                                          label  wcount  \n",
       "postid                                                                   \n",
       "1015a0f4961d  [False, True, False, False, False, False, Fals...     522  \n",
       "101a407e8c61  [False, False, False, False, False, False, Fal...     393  \n",
       "1030d29376f1  [False, False, False, False, False, False, Fal...     850  \n",
       "10315016b299  [False, False, False, False, False, False, Fal...     381  \n",
       "10321e751c6d  [False, False, False, False, False, False, Fal...     993  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sentence-wise split:</b> We set up the data by sentence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a dataframe for sentences...\n",
    "# dfS = pd.DataFrame()\n",
    "\n",
    "# for art in dfTrain.index:\n",
    "#     slist = dfTrain.text[art]\n",
    "#     for nsent in range(len(slist)):\n",
    "#         sent = slist[nsent]\n",
    "#         dfRow = pd.DataFrame([art,sent,len(sent),nsent,len(slist),dfTrain.label[art][nsent]])\n",
    "#         dfRow = dfRow.T\n",
    "#         dfRow.columns = ['postid','sentence','swcount','sposition','alength','slabel']\n",
    "#         dfS = pd.concat([dfS,dfRow])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434741, 7)\n"
     ]
    }
   ],
   "source": [
    "# grab dfS from database\n",
    "\n",
    "dfS = pd.read_sql('sentences_train', dbe)\n",
    "print(dfS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434741, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>postid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>swcount</th>\n",
       "      <th>sposition</th>\n",
       "      <th>alength</th>\n",
       "      <th>slabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{hola,desde,cuba}</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{today,air,force,one,touched,havana,first,time...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{question,remarkable,moment,relationship,unite...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{also,landmark,progress,made,since,president,o...</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>{trip,also,professionally,personally,meaningfu...</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        postid                                           sentence  \\\n",
       "0      0  1015a0f4961d                                  {hola,desde,cuba}   \n",
       "1      0  1015a0f4961d  {today,air,force,one,touched,havana,first,time...   \n",
       "2      0  1015a0f4961d  {question,remarkable,moment,relationship,unite...   \n",
       "3      0  1015a0f4961d  {also,landmark,progress,made,since,president,o...   \n",
       "4      0  1015a0f4961d  {trip,also,professionally,personally,meaningfu...   \n",
       "\n",
       "   swcount  sposition  alength  slabel  \n",
       "0        3          0       52   False  \n",
       "1        9          1       52    True  \n",
       "2        9          2       52   False  \n",
       "3       29          3       52   False  \n",
       "4       19          4       52   False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dfS.shape)\n",
    "dfS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfS.to_sql('sentences_train',dbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1457, 15)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, add sentence entries for the highlights not in ptext.\n",
    "\n",
    "h_in_ptext = [any(a) for a in plabel]\n",
    "dfNotInP = dfDrop.drop(dfDrop.index[h_in_ptext])\n",
    "dfNotInP.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dfComb = dfS.copy()\n",
    "\n",
    "# for art in dfNotInP.index:\n",
    "#     slist = dfNotInP.text[art]\n",
    "#     for nsent in range(len(slist)):\n",
    "#         sent = slist[nsent]\n",
    "#         dfRow = pd.DataFrame([art,sent,len(sent),nsent,len(slist),dfNotInP.label[art][nsent]])\n",
    "#         dfRow = dfRow.T\n",
    "#         dfRow.columns = ['postid','sentence','swcount','sposition','alength','slabel']\n",
    "#         dfComb = pd.concat([dfComb,dfRow])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfComb.to_sql('sentences_train_addnotinp',dbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564102, 6)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dfComb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adding article data:</b> We perform a merge to add article-wise data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434741, 11)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index        False\n",
       "postid       False\n",
       "sentence     False\n",
       "swcount      False\n",
       "sposition    False\n",
       "alength      False\n",
       "slabel       False\n",
       "title        False\n",
       "nlikes       False\n",
       "npar         False\n",
       "wcount       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# left merge dfS -> dfTrain on postid\n",
    "\n",
    "dfX = pd.merge(dfS, dfTrain[['title','nlikes','npar','wcount']], \n",
    "               how='left', left_on='postid', left_index=False, right_index=True, sort=False)\n",
    "print(dfX.shape)\n",
    "nsamp = dfX.shape[0]\n",
    "dfX.isnull().any()\n",
    "#dfX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 'ncomments' column has a bunch of NaNs, so here we have dropped it. Later we will investigate why (is it the dumb `convert_str()` function?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434741"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX = dfX.dropna(how='any')\n",
    "dfX.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we set up the model itself.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scikitlearn logistic regression... fit (with 2/3 of X, y from above)\n",
    "\n",
    "dfY = dfX['slabel']\n",
    "dfX = dfX[['swcount','sposition','alength','nlikes','wcount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289827,)\n",
      "(289827, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl = math.floor(2*nsamp/3)\n",
    "ytrain = dfY.iloc[0:spl].astype(int)\n",
    "Xtrain = dfX.iloc[0:spl]\n",
    "ytest = dfY.iloc[spl:].astype(int)\n",
    "Xtest = dfX.iloc[spl:]\n",
    "print(ytrain.shape)\n",
    "print(Xtrain.shape)\n",
    "lrm = linear_model.LogisticRegression()\n",
    "lrm.fit(Xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98661270344 0.98661270344\n",
      "0.985922685179 0.985922685179\n"
     ]
    }
   ],
   "source": [
    "# test on the 2/3 of training set\n",
    "print(lrm.score(Xtrain,ytrain), 1 - ytrain.mean())\n",
    "\n",
    "# test on the other 1/3 of X, y\n",
    "print(lrm.score(Xtest,ytest), 1 - ytest.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is predicting \"no highlight\" for every sample. This is what I expected, given the extremely unbalanced nature of the data. I will try balancing techniques (just as soon as I get Flask to work...)\n",
    "<br><br>\n",
    "Okay. Flask works. Now I need a predicting model.\n",
    "<br><br>\n",
    "<ul>\n",
    "<li>The very first thing to do is to add the other positive examples I am missing. Find the instances of highlights not included in the ptext adn add them to the dfS. <i><b>Done above, resulting in dfComb</b></i></li>\n",
    "<li>THEN add the new dfS to the databse!!</li>\n",
    "<li>The next thing, I think, is to try the tf-idf embedding via <b>gensim</b>. Cluster the words based on term frequency v. inverse article frequency, then choose clusters that correspond the the first few hundred eigenvectors of the basis.</li>\n",
    "<li>Once you have done that, </li>\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make ROC curve to compare thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adding New Features</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is getting the text by article and making tf-idf vectors.\n",
    "<ol>\n",
    "<li>Text by article is created by joining sentences. </li>\n",
    "<li>Frequency count of each word in the whole corpus. </li>\n",
    "<li>Remove words that appear only once. </li>\n",
    "<li>Create a dictionary and a corpus. </li>\n",
    "<li></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text by article\n",
    "def to_atext(art):\n",
    "    ntext = []\n",
    "    for s in art:\n",
    "        ntext.extend(s)\n",
    "    return ntext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'desde', 'cuba', 'today', 'air', 'force', 'one', 'touched', 'havana', 'first', 'time', 'history', 'question', 'remarkable', 'moment', 'relationship', 'united', 'states', 'cuba', 'governments', 'people', 'also', 'landmark', 'progress', 'made', 'since', 'president', 'obama', 'decided', 'reform', 'failed', 'cold', 'war', 'era', 'policies', 'past', 'chart', 'new', 'course', 'would', 'actually', 'advance', 'american', 'interests', 'values', 'help', 'cuban', 'people', 'improve', 'lives', 'trip', 'also', 'professionally', 'personally', 'meaningful', 'special', 'assistant', 'advisor', 'antoinette', 'rangel', 'cuban', 'american', 'learned', 'country', 'stories', 'abuela', 'maria', 'shared', 'growing', 'good', 'illustration', 'closely', 'two', 'countries', 'linked', 'trip', 'potential', 'change', 'lives', 'families', 'cuba', 'united', 'states', 'looking', 'forward', 'learning', 'first', 'hand', 'cuban', 'culture', 'life', 'bringing', 'white', 'house', 'press', 'corps', 'along', 'ride', 'daily', 'basis', 'answering', 'questions', 'president', 'trip', 'cuba', 'pose', 'question', 'twitter', 'using', 'askpresssec', 'answer', 'presssec', 'ground', 'let', 'start', 'questions', 'know', 'many', 'americans', 'cuba', 'visit', 'q', 'president', 'decide', 'change', 'u', 'policy', 'toward', 'cuba', 'fifty', 'years', 'united', 'states', 'wedded', 'policy', 'isolate', 'pressure', 'cuba', 'without', 'seeing', 'results', 'cuba', 'political', 'system', 'change', 'making', 'life', 'better', 'cuban', 'people', 'citing', 'lack', 'progress', 'december', '17', '2014', 'president', 'announced', 'country', 'policy', 'toward', 'cuba', 'changing', 'course', 'since', 'made', 'substantial', 'progress', 'normalizing', 'relations', 'pleased', 'see', 'cuban', 'people', 'overwhelmingly', 'support', 'new', 'policy', 'fact', 'democrats', 'republicans', 'congress', 'joining', 'president', 'trip', 'good', 'illustration', 'strong', 'bipartisan', 'support', 'new', 'opening', 'cuba', 'opened', 'embassy', 'havana', '50', 'years', 'shuttered', 'doors', 'expanded', 'commercial', 'ties', 'made', 'easier', 'americans', 'travel', 'business', 'restored', 'direct', 'flights', 'direct', 'mail', 'case', 'missed', 'check', 'president', 'obama', 'letter', '76', 'year', 'old', 'woman', 'cuba', 'carried', 'first', 'mail', 'flight', '50', 'years', 'significant', 'changes', 'welcome', 'start', 'important', 'mission', 'expanding', 'people', 'people', 'interaction', 'commercial', 'enterprise', 'today', 'americans', 'visiting', 'cuba', 'time', 'last', '50', 'years', 'cuban', 'american', 'families', 'american', 'students', 'volunteers', 'faith', 'leaders', 'entrepreneurs', 'president', 'trip', 'big', 'opportunity', 'advance', 'progress', 'beginning', 'new', 'phase', 'progress', 'reflects', 'interests', 'values', 'better', 'future', 'cuban', 'people', 'take', 'time', 'path', 'president', 'decided', 'time', 'take', 'necessary', 'steps', 'toward', 'better', 'future', 'citizens', 'countries', 'q', 'cuba', 'short', 'trip', 'lot', 'get', 'done', 'colleague', 'ben', 'rhodes', 'president', 'deputy', 'national', 'security', 'advisor', 'along', 'help', 'senior', 'advisor', 'bernadette', 'meehan', 'taking', 'lead', 'implementing', 'policy', 'negotiating', 'cuban', 'government', 'helpfully', 'laid', 'marquee', 'events', 'president', 'scheduled', 'highlights', 'sunday', 'taking', 'walking', 'tour', 'old', 'havana', 'met', 'carinal', 'ortega', 'latin', 'rite', 'archbishop', 'archdiocese', 'havana', 'cardinal', 'catholic', 'church', 'tour', 'places', 'illustrate', 'history', 'cultural', 'significance', 'beautiful', 'city', 'monday', 'lay', 'wreath', 'jose', 'marti', 'memorial', 'participate', 'discussion', 'entrepreneurship', 'opportunity', 'cuentapropistas', 'cuban', 'entrepreneurs', 'also', 'head', 'revolutionary', 'palace', 'meet', 'president', 'raÃºl', 'castro', 'discuss', 'together', 'make', 'easier', 'trade', 'easier', 'cubans', 'access', 'internet', 'start', 'businesses', 'tuesday', 'first', 'family', 'attend', 'baseball', 'game', 'tampa', 'bay', 'rays', 'cuban', 'national', 'team', 'latinoamerican', 'stadium', 'first', 'time', 'mlb', 'team', 'played', 'cuba', 'since', '1999', 'tampa', 'bay', 'rays', 'lottery', 'pick', 'among', 'teams', 'wanted', 'come', 'play', 'one', 'things', 'sure', 'royals', 'come', 'next', 'coming', 'q', 'president', 'hope', 'see', 'happen', 'cuba', 'visit', 'president', 'change', 'policy', 'historic', 'visit', 'come', 'belief', 'u', 'help', 'make', 'difference', 'cuban', 'people', 'changing', 'way', 'americans', 'engage', 'cubans', 'foster', 'hope', 'future', 'making', 'president', 'policies', 'geared', 'toward', 'providing', 'opportunity', 'cuban', 'people', 'rather', 'isolating', 'past', 'course', 'real', 'differences', 'political', 'economic', 'systems', 'difficult', 'histories', 'prevented', 'progress', 'historic', 'trip', 'president', 'hopes', 'share', 'vision', 'americans', 'cubans', 'together', 'ensure', 'future', 'cuba', 'reflects', 'freedom', 'opportunity', 'people', 'good', 'cuba', 'good', 'america', 'especially', 'millions', 'americans', 'deep', 'enduring', 'ties', 'cuba', 'follow', 'along', 'follow', 'presssec', 'twitter', 'explore', 'cuba', 'future', 'build', 'together', 'trip', 'remember', 'account', 'maintained', 'national', 'archives', 'records', 'administration', 'nara', 'serve', 'archive', 'obama', 'administration', 'content']\n"
     ]
    }
   ],
   "source": [
    "# flatten second row\n",
    "atext = [to_atext(art) for art in ptext]\n",
    "print(atext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do stemming with porter stemmer\n",
    "porter = PorterStemmer()\n",
    "atext = [[porter.stem(t) for t in art] for art in atext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n"
     ]
    }
   ],
   "source": [
    "htext = [[[porter.stem(w) for w in s] for s in hil] for hil in htext]\n",
    "ttext = [[[porter.stem(w) for w in s] for s in tit] for tit in ttext]\n",
    "print(htext[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have flattened the text within each document and stemmed each word, we will go on to frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gensim #(above...)\n",
    "\n",
    "# find frequency of each word in corpus (article list)\n",
    "frequency = defaultdict(int)\n",
    "for subtext in atext:\n",
    "    for token in subtext:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words/stems with frequency of 1\n",
    "atext = [[token for token in art if frequency[token] > 1]\n",
    "         for art in atext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a <b>dictionary</b>, which is a word-frequency mapping space of n (= number of words) dimensions. It spans the bag-of-words space of our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we create a dictionary\n",
    "dictionary = corpora.Dictionary(atext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n",
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n"
     ]
    }
   ],
   "source": [
    "# practice representing a highlight in this dictionary space\n",
    "test_doc = htext[0][0]\n",
    "print(test_doc)\n",
    "new_vec = dictionary.doc2bow(test_doc)\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a corpus\n",
    "output = open('atext.pkl', 'wb')\n",
    "pickle.dump(atext, output)\n",
    "output.close()\n",
    "\n",
    "o2 = open('dicty.pkl', 'wb')\n",
    "pickle.dump(dictionary, o2)\n",
    "o2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the corpus elsewhere\n",
    "# read in\n",
    "#from gensim_corpus import get_corpus\n",
    "#corp = get_corpus('atext_train_corp.mm')\n",
    "corp = corpora.MmCorpus('atext_train_corp.mm')\n",
    "#print(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n",
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "[(2, 0.1217169239582691), (3, 0.30230312910390755), (4, 0.1828764398150676), (5, 0.017012819573171706), (6, 0.2557849334233673), (7, 0.8665596984748893), (8, 0.05368257118654773), (9, 0.019686602255689127), (10, 0.20110583368465484)]\n"
     ]
    }
   ],
   "source": [
    "# create tfidf model\n",
    "tfidf = models.TfidfModel(corp)\n",
    "corp_tfidf = tfidf[corp]\n",
    "print(htext[0][0])\n",
    "print(new_vec) # dictionary space\n",
    "print(tfidf[new_vec]) # tf-idf space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a tf-idf model, we can rank each sentence by the importance of its words. Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'desde', 'cuba']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptext[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'desde', 'cuba']\n",
      "[(1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# for each sentence, calculate the tf-idf score (average over sentence words)\n",
    "sentence_corp = []\n",
    "for art in ptext:\n",
    "    for sent in art:\n",
    "        sentence_corp.append( dictionary.doc2bow(sent) )\n",
    "\n",
    "print(ptext[0][0])\n",
    "print(sentence_corp[0])\n",
    "sentence_tfidf = tfidf[sentence_corp]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/anaconda3/envs/insight_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/kimberly/anaconda3/envs/insight_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# stfidf feature\n",
    "stfidf = []\n",
    "for sent_rep in sentence_tfidf:\n",
    "    try:\n",
    "        stfidf.append( np.mean([wr[1] for wr in sent_rep]) )\n",
    "    except:\n",
    "        stfidf.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403043\n",
      "434792\n",
      "434792\n"
     ]
    }
   ],
   "source": [
    "print(len(np.argwhere(~np.isnan(stfidf))))\n",
    "print(len(stfidf))\n",
    "print(len(sentence_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3031c6e8fd1d\n",
      "434741\n",
      "434741\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# make a vector with the nans for the dropped article\n",
    "print(postidToIgnore)\n",
    "\n",
    "# splx = np.cumsum([len(art) for ix,art in enumerate(ptext) if ix<637])[-1]\n",
    "\n",
    "# tmp = [s for ix,s in enumerate(stfidf) if ix<splx]\n",
    "# tmp.extend([None]*len(ptext[637]))\n",
    "# tmp.extend([s for ix,s in enumerate(stfidf) if ix>splx])\n",
    "# stfidf = tmp\n",
    "\n",
    "tmp = []\n",
    "for ax in range(len(ptext)):\n",
    "    if ax==637:\n",
    "        tmp.extend([])\n",
    "    else:\n",
    "        cpy = copy.deepcopy(stfidf[0:len(ptext[ax])])\n",
    "        tmp.extend(cpy)\n",
    "        stfidf = stfidf[len(art):]\n",
    "        \n",
    "        \n",
    "\n",
    "print(len(tmp))\n",
    "stfidf = tmp\n",
    "print(len(stfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>THERE IS A NEW FEATURE, BOOYA</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM7PCOKiYmVlhHFTMzKwwDipmZlaY/w8XA4EZHPKsQQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ed548d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the tf-idf distribution over sentences\n",
    "\n",
    "\n",
    "# the histogram of the data\n",
    "ax = plt.hist([x for x in stfidf if str(x) != 'nan'], 50)\n",
    "\n",
    "plt.xlabel('tf-idf value of sentence')\n",
    "plt.ylabel('number of sentences')\n",
    "plt.title('tf-idf dist over sentences')\n",
    "#labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "#ax.set_xticklabels(labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add a topic model. Our goal is to pare down the tf-idf space into a subspace defined by major topic axes, and use topics as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "[(2, 0.1217169239582691), (3, 0.30230312910390755), (4, 0.1828764398150676), (5, 0.017012819573171706), (6, 0.2557849334233673), (7, 0.8665596984748893), (8, 0.05368257118654773), (9, 0.019686602255689127), (10, 0.20110583368465484)]\n",
      "[(0, 0.21326115384176877), (1, -0.050753138377643905), (2, -0.01165201352036738), (3, 0.0020469609691552192), (4, -0.017301212144670931), (5, -0.027376997508276678), (6, -0.011562781642030066), (7, -0.001005163567988362), (8, -0.044062716864880939), (9, -0.049193435825593913), (10, 0.026420692561905934), (11, -0.045398465626371842), (12, 0.013754907461564865), (13, -0.031798823409963442), (14, -0.0061805796860579798), (15, -0.026447129453630383), (16, -0.048739920296070494), (17, -0.019594156861315933), (18, 0.0094980878608990453), (19, 0.0045841838544453516), (20, 0.0010834311895850128), (21, -0.049940884917560595), (22, -0.0065062212192689194), (23, 0.0012399845513321843), (24, -0.0033549768380634337), (25, 0.0049623550463525606), (26, 0.028458485633294475), (27, -0.00014049999284886962), (28, 0.020792921869365478), (29, -0.01248796259027448), (30, 0.0069502393207835824), (31, -0.0092208929456301346), (32, 0.081692401195319453), (33, -0.020315256525273599), (34, -0.049815239985515201), (35, 0.039035674516161634), (36, 0.02257963831467261), (37, -0.013901263022884847), (38, -0.032709259059480869), (39, 0.01079759119820543), (40, -0.0090238835740462774), (41, -0.0077368720623499435), (42, 0.024548036133011771), (43, -0.006627812068340469), (44, 0.04055803129205976), (45, -0.020394423596002808), (46, -0.002731407048222982), (47, -0.031086321327015984), (48, 0.010584445599995907), (49, -0.022843327921797673), (50, -0.0077832055042150478), (51, -0.039671220705341272), (52, -0.0196299682456079), (53, 0.012661105160323944), (54, 0.012841591765630905), (55, -0.024003577787799522), (56, -0.041605973458077192), (57, 0.013787076392816995), (58, 0.0019398702095889241), (59, -0.029730844045250303), (60, -0.039266890545504023), (61, 0.0021239669862601392), (62, -0.0088100774326421178), (63, -0.003316150314065965), (64, -0.0082957897958001285), (65, 0.016966672426433363), (66, -0.00095575722507668277), (67, 0.031493668918526056), (68, 0.013626500495490751), (69, 0.038624579707373485), (70, 0.028695565702158969), (71, 0.035341286363276193), (72, 0.011706360551817199), (73, 0.0032464734860961557), (74, 0.021971306488081949), (75, -0.00229381331162367), (76, -0.045797064564283396), (77, -0.027876819263375145), (78, -0.040221576832329596), (79, -0.026098890232035609), (80, 0.0054862311786031326), (81, -0.087666353620636545), (82, -0.047118654775386459), (83, 0.0045912094797092712), (84, -0.047236675710942429), (85, 0.014798194987283644), (86, -0.038326522925081721), (87, -0.024577703164718561), (88, 0.021242238423929166), (89, 0.040242898743543198), (90, -0.012856408017338894), (91, 0.0082551580305352916), (92, -0.019938069567657271), (93, -0.026536998952073836), (94, -0.011959068892596496), (95, 0.024270041490396781), (96, 0.0037012172077839894), (97, -0.0068247707782943864), (98, 0.028309131851303897), (99, 0.00096042976332648151), (100, -0.054132040753484489), (101, -0.029108827806550341), (102, -0.0083644112253749327), (103, 0.013908116915992103), (104, -0.030447743379590369), (105, 0.0044489415845082973), (106, 0.0066930052355267632), (107, 0.022727747290891231), (108, -0.038811466681207392), (109, 0.027263615100835532), (110, 0.053329413477464963), (111, -0.0074705772364052035), (112, -0.036192981285884746), (113, -0.01442466111624716), (114, 0.028609273700914904), (115, -0.016668162164816078), (116, 0.035955036112337371), (117, -0.030021826883772119), (118, -0.050164460597058126), (119, -0.030290815570334018), (120, -0.030397861821122564), (121, 0.011978873135469898), (122, 0.0065839758342511098), (123, -0.010844171663295018), (124, 0.041594629839181059), (125, 0.039615038173971912), (126, 0.0053061610643460096), (127, 0.00076809995603762359), (128, 0.028458802247045029), (129, 0.0053979260503540481), (130, -0.037706855194950574), (131, 0.040060256793850724), (132, -0.0058247724281199394), (133, -0.010163143480949093), (134, 0.034730757469000548), (135, -0.020023396089311564), (136, 0.053765504457748324), (137, 0.010290583022469445), (138, -0.023614990581043795), (139, 0.02055537212600117), (140, -0.035834868212280893), (141, -0.023738177252174843), (142, -0.020908554792455077), (143, 0.054433263023326565), (144, 0.043654073679099535), (145, 0.014936743931307233), (146, -0.075548564323074288), (147, -0.069773674437966393), (148, 0.054471595126506026), (149, -0.0085794139893943626), (150, 0.033468930498845015), (151, 0.0098102817700866721), (152, -0.027358725165996467), (153, -0.012969483842410586), (154, -0.010355316875018339), (155, 0.051559805978913797), (156, 0.019972541635422683), (157, -0.0038563062998910533), (158, 0.069428127959085775), (159, -0.022273662624456528), (160, -0.022766010651171627), (161, 0.018617402789022831), (162, 0.063002140851265029), (163, -0.028358077567137711), (164, 0.0072101257454030487), (165, 0.0066288341648839434), (166, 0.014356607099952852), (167, 0.074364004641113091), (168, -0.053837809812710587), (169, -0.0038251814847167368), (170, 0.0087092078706725862), (171, 0.0055980332609151945), (172, 0.019005890278210671), (173, 0.066123617730182396), (174, -0.01192559166396317), (175, 0.035867612819544065), (176, 0.033988027560497139), (177, -0.048338931794635651), (178, -0.019029550017974756), (179, 0.023218920358402305), (180, -0.028670602222927766), (181, -0.068356467165677323), (182, -0.0040789633002745922), (183, 0.021363266607888183), (184, 0.0085144967341556008), (185, -0.024374042786373427), (186, 0.011437202735781662), (187, 0.066399375970102495), (188, -0.034045065990376086), (189, -0.0013867415813588755), (190, -0.067042656877932122), (191, 0.021120637803683062), (192, -0.038107832400374624), (193, 0.025227216021407414), (194, -0.051762100360350602), (195, -0.015626337083975685), (196, 0.036638456984275598), (197, -0.02777059212619807), (198, 0.032515878126168664), (199, -0.012905063472647285)]\n"
     ]
    }
   ],
   "source": [
    "# create LSI model (try LDA later?)\n",
    "lsi = models.LsiModel(corp_tfidf, id2word=dictionary, num_topics=200)\n",
    "corp_lsi = lsi[corp_tfidf]\n",
    "sentence_lsi = lsi[sentence_corp]\n",
    "print(new_vec)          # in original dictionary\n",
    "print(tfidf[new_vec])   # in tf-idf space\n",
    "print(lsi[new_vec])     # in LSI/LSA topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.098*\"design\" + 0.082*\"life\" + 0.079*\"book\" + 0.077*\"user\" + 0.075*\"product\" + 0.074*\"compani\" + 0.070*\"learn\" + 0.068*\"success\" + 0.066*\"app\" + 0.065*\"busi\"'),\n",
       " (1,\n",
       "  '0.322*\"user\" + 0.306*\"design\" + -0.237*\"trump\" + 0.219*\"app\" + 0.138*\"ux\" + 0.114*\"code\" + 0.104*\"ui\" + -0.101*\"women\" + -0.096*\"presid\" + 0.095*\"product\"'),\n",
       " (2,\n",
       "  '-0.407*\"trump\" + -0.165*\"user\" + -0.150*\"presid\" + -0.150*\"clinton\" + -0.143*\"vote\" + -0.133*\"elect\" + -0.132*\"hillari\" + -0.117*\"design\" + 0.111*\"book\" + -0.109*\"women\"'),\n",
       " (3,\n",
       "  '-0.575*\"que\" + -0.395*\"de\" + -0.253*\"nÃ£o\" + -0.230*\"Ã©\" + -0.217*\"e\" + -0.150*\"da\" + -0.148*\"um\" + -0.143*\"uma\" + -0.140*\"para\" + -0.121*\"se\"'),\n",
       " (4,\n",
       "  '0.276*\"creatom\" + 0.238*\"jon\" + 0.177*\"startup\" + 0.174*\"compani\" + 0.168*\"agenc\" + -0.138*\"habit\" + 0.127*\"market\" + 0.127*\"gl\" + 0.127*\"goo\" + 0.123*\"westenberg\"'),\n",
       " (5,\n",
       "  '0.384*\"trump\" + -0.264*\"women\" + -0.175*\"men\" + -0.162*\"black\" + 0.113*\"clinton\" + -0.111*\"woman\" + -0.110*\"girl\" + -0.104*\"white\" + 0.100*\"presid\" + 0.092*\"success\"'),\n",
       " (6,\n",
       "  '-0.325*\"code\" + 0.276*\"design\" + 0.266*\"user\" + -0.238*\"javascript\" + 0.167*\"ux\" + 0.156*\"creatom\" + -0.142*\"develop\" + -0.139*\"program\" + -0.138*\"react\" + 0.133*\"jon\"'),\n",
       " (7,\n",
       "  '0.300*\"basecamp\" + -0.226*\"creatom\" + -0.209*\"code\" + 0.208*\"compani\" + -0.191*\"javascript\" + -0.176*\"jon\" + -0.149*\"react\" + -0.126*\"agenc\" + 0.121*\"employe\" + 0.117*\"custom\"'),\n",
       " (8,\n",
       "  '0.391*\"design\" + -0.333*\"app\" + 0.240*\"basecamp\" + -0.171*\"appl\" + -0.162*\"user\" + 0.142*\"code\" + 0.130*\"sketch\" + -0.127*\"snapchat\" + 0.119*\"color\" + -0.113*\"facebook\"'),\n",
       " (9,\n",
       "  '-0.439*\"basecamp\" + 0.233*\"women\" + -0.187*\"book\" + -0.145*\"trump\" + 0.125*\"men\" + 0.114*\"user\" + -0.109*\"write\" + -0.106*\"read\" + -0.102*\"rework\" + -0.100*\"1999\"'),\n",
       " (10,\n",
       "  '-0.391*\"book\" + 0.314*\"basecamp\" + -0.190*\"read\" + 0.185*\"app\" + -0.147*\"design\" + -0.139*\"medium\" + -0.125*\"write\" + -0.113*\"writer\" + 0.106*\"user\" + 0.105*\"react\"'),\n",
       " (11,\n",
       "  '0.464*\"women\" + 0.242*\"basecamp\" + 0.239*\"men\" + 0.134*\"task\" + 0.130*\"habit\" + 0.128*\"woman\" + 0.125*\"app\" + -0.105*\"lebron\" + 0.103*\"book\" + 0.094*\"male\"'),\n",
       " (12,\n",
       "  '-0.280*\"user\" + -0.268*\"basecamp\" + 0.199*\"sketch\" + 0.190*\"design\" + -0.152*\"book\" + 0.145*\"task\" + 0.139*\"morn\" + 0.128*\"sleep\" + 0.127*\"team\" + 0.117*\"habit\"'),\n",
       " (13,\n",
       "  '-0.397*\"black\" + -0.251*\"white\" + -0.237*\"color\" + 0.218*\"user\" + 0.180*\"women\" + 0.159*\"trump\" + -0.150*\"polic\" + -0.142*\"sketch\" + -0.131*\"basecamp\" + -0.125*\"postanli\"'),\n",
       " (14,\n",
       "  '0.260*\"book\" + -0.193*\"app\" + -0.176*\"postanli\" + 0.132*\"creatom\" + 0.120*\"black\" + 0.118*\"read\" + -0.117*\"trump\" + 0.099*\"basecamp\" + -0.098*\"sketch\" + 0.098*\"sleep\"'),\n",
       " (15,\n",
       "  '0.266*\"sketch\" + 0.234*\"compon\" + -0.226*\"code\" + 0.204*\"react\" + -0.201*\"black\" + -0.161*\"user\" + 0.159*\"women\" + -0.119*\"program\" + -0.115*\"white\" + -0.111*\"learn\"'),\n",
       " (16,\n",
       "  '-0.338*\"book\" + -0.334*\"appl\" + 0.258*\"medium\" + -0.224*\"app\" + 0.141*\"content\" + 0.130*\"media\" + -0.120*\"iphon\" + 0.116*\"snapchat\" + -0.116*\"women\" + 0.114*\"facebook\"'),\n",
       " (17,\n",
       "  '-0.288*\"lebron\" + -0.198*\"durant\" + -0.187*\"nba\" + 0.177*\"appl\" + -0.174*\"black\" + -0.166*\"team\" + -0.161*\"game\" + -0.150*\"player\" + -0.148*\"user\" + 0.133*\"sketch\"'),\n",
       " (18,\n",
       "  '-0.283*\"black\" + 0.197*\"lebron\" + 0.176*\"women\" + -0.172*\"white\" + 0.134*\"durant\" + 0.133*\"game\" + 0.131*\"basecamp\" + -0.129*\"react\" + -0.128*\"slack\" + 0.127*\"medium\"'),\n",
       " (19,\n",
       "  '0.279*\"slack\" + -0.225*\"color\" + 0.173*\"bot\" + 0.163*\"team\" + -0.128*\"money\" + 0.123*\"design\" + -0.122*\"postanli\" + 0.120*\"app\" + -0.106*\"inc\" + -0.106*\"wordstream\"')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the most important topics in LSI\n",
    "lsi.print_topics(20)\n",
    "#[print(doc) for ndoc,doc in enumerate(corpus_lsi) if ndoc<20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get each sentence's main topic. Do they cluster by article or not?\n",
    "slsi = []\n",
    "for sent_rep in sentence_lsi:\n",
    "    #slsi = mean([wr[1] for wr in sent_rep])  # this needs to pick out a main topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HERE will be a plot that shows whether the sentence topics cluster by article.\n",
    "# 2D space. Use 10 articles to start. Different color for article.\n",
    "#Make the scatter plot.....\n",
    "sentence_lsi = lsi[sentence_corp]\n",
    "print(len(sentence_lsi))\n",
    "\n",
    "# OR try 1D space and have D2 be the article ID.\n",
    "#print(sentence_lsi[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358585\n"
     ]
    }
   ],
   "source": [
    "# convert sentence_lsi to the right length\n",
    "print(len(sentence_lsi))\n",
    "\n",
    "# tmp = []\n",
    "# for ax in range(len(ptext)):\n",
    "#     if ax==637:\n",
    "#         tmp.extend([])\n",
    "#     else:\n",
    "#         cpy = copy.deepcopy(sentence_lsi[0:len(ptext[ax])])\n",
    "#         tmp.extend(cpy)\n",
    "#         sentence_lsi = sentence_lsi[len(ptext[ax]):]\n",
    "        \n",
    "# sentence_lsi = tmp\n",
    "dfLsi = pd.DataFrame(sentence_lsi)\n",
    "#tmp = []\n",
    "#[tmp.extend(dfS.sentence_lsi[dfS.article_index==ax],sim_index,ax)\n",
    "# for ax in range(len(ptext)) if ax!=637]\n",
    "\n",
    "print(len(sentence_lsi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, we set up for similarity queries \n",
    "sim_index = similarities.Similarity('.',lsi[corp],num_features = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n",
      "[ 0.07958278 -0.03962181  0.16909875 ..., -0.15677771 -0.00896206\n",
      " -0.21422096]\n",
      "[ 0.07958278 -0.03962181  0.16909875 ..., -0.15677771 -0.00896206\n",
      " -0.21422096]\n"
     ]
    }
   ],
   "source": [
    "# find similarity between one highlight and each article\n",
    "h_sim = sim_index[new_vec,new_vec]\n",
    "#sorted_sims = sorted(enumerate(h_sim), key=lambda item: -item[1])\n",
    "sorted_sims[0:10]\n",
    "print(test_doc)\n",
    "[ttext[ix] for ix,csim in sorted_sims[1:10]]\n",
    "print(h_sim[0])\n",
    "print(h_sim[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.lsimodel.LsiModel'>\n",
      "[0.62952983, 0.44973826, 0.35316467, 0.72799098, 0.67485505, 0.52800846, 0.33686295, 0.53076595, 0.21482013, 0.49457726, 0.34945545, 0.58120322, 0.0, 0.58626425, 0.47049266, 0.52747184, 0.54961306, 0.58827513, 0.4156751, 0.26211897, 0.19207311, 0.5217998, 0.12827414, 0.69241178, 0.32494941, 0.28505197, 0.56771523, 0.28566062, 0.59970832, 0.23064187, 0.53208011, 0.13968758, 0.0, 0.37813577, 0.35948831, 0.26094598, 0.28673223, 0.39448646, 0.32668346, 0.5243991, 0.37928376, 0.56662709, 0.2302499, 0.53924632, 0.071148254, 0.26855862, 0.46432236, 0.52663159, 0.44722167, 0.30501544, 0.22526461, 0.40926719]\n"
     ]
    }
   ],
   "source": [
    "# find cosine similarity of each sentence to its own article\n",
    "#dfS.head()\n",
    "\n",
    "#print(type(lsi))\n",
    "\n",
    "# sent_own_sims = []\n",
    "# for ax in range(len(ptext)):\n",
    "#     if ax==0 or ax==1:#ax!=637:\n",
    "#         s_sims = sim_sent_own_doc(ptext[ax],sim_index,ax,dictionary,lsi)\n",
    "#         sent_own_sims.extend( s_sims )\n",
    "\n",
    "#[sent_own_sims.extend(sim_sent_own_doc(ptext[ax],sim_index,ax,dictionary,lsi)) \n",
    "#for ax in range(len(ptext)) if ax!=637]        \n",
    "        \n",
    "#dfS[['postid','sentence']]\n",
    "print(len(sent_own_sims))\n",
    "print(sent_own_sims[0])\n",
    "#print(ptext[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_sent_own_doc(docsents,index,docpos,dicty,mod):\n",
    "    '''finds similarity of each sentence to its own doc'''\n",
    "    a_lsi = [mod[dicty.doc2bow(s)] for s in docsents]\n",
    "    sims = index[a_lsi]\n",
    "    sims = [s[docpos] for s in sims]\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_own_sim(sent_reps,index,docpos):\n",
    "    '''finds similarity of ddivs (ddiv_list is list fo sents with same doc) to own doc'''\n",
    "    sims = index[list(sent_reps)]\n",
    "    sims = [s[docpos] for s in sims]\n",
    "    return sims\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434792\n"
     ]
    }
   ],
   "source": [
    "# indices of sentences for each \n",
    "len(ptext)\n",
    "sum([len(ptext[ax]) for ax in range(len(ptext))])# if ax != 637])\n",
    "s_ix = [[ax]*len(ptext[ax]) for ax in range(len(ptext))]# if ax != 637]\n",
    "s_ix = [s for a in s_ix for s in a]\n",
    "print(len(s_ix))\n",
    "#s_ix[0:53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfS['article_index'] = [i for i in s_ix if i!=637]\n",
    "dfS['sentence_lsi'] = sentence_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine similarity of sentence to title\n",
    "final_sent_sims = []\n",
    "\n",
    "\n",
    "\n",
    "#sreps = [sentence_lsi[s_ix[ax]] for ax in range(len(ptext)) if ax!=637]\n",
    "final_sent_sims = [sentence_own_sim(dfS.sentence_lsi[dfS.article_index==ax],sim_index,ax)\n",
    " for ax in range(len(ptext)) if ax!=637]\n",
    "    \n",
    "final_sent_sims = [s for a in final_sent_sims for s in a]\n",
    "    \n",
    "print(len(final_sent_sims))\n",
    "final_sent_sims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>swcount</th>\n",
       "      <th>sposition</th>\n",
       "      <th>alength</th>\n",
       "      <th>slabel</th>\n",
       "      <th>article_index</th>\n",
       "      <th>sentence_lsi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[hola, desde, cuba]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 0.00183121283686), (1, 0.00408547304655),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[today, air, force, one, touched, havana, firs...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 0.154857874989), (1, 0.0239465394874), (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[question, remarkable, moment, relationship, u...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 0.118441694899), (1, 0.0393959066023), (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[also, landmark, progress, made, since, presid...</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 0.359657148598), (1, 0.16882466926), (2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[trip, also, professionally, personally, meani...</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0, 0.102471883439), (1, 0.0783365336689), (2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         postid                                           sentence swcount  \\\n",
       "0  1015a0f4961d                                [hola, desde, cuba]       3   \n",
       "0  1015a0f4961d  [today, air, force, one, touched, havana, firs...       9   \n",
       "0  1015a0f4961d  [question, remarkable, moment, relationship, u...       9   \n",
       "0  1015a0f4961d  [also, landmark, progress, made, since, presid...      29   \n",
       "0  1015a0f4961d  [trip, also, professionally, personally, meani...      19   \n",
       "\n",
       "  sposition alength slabel  article_index  \\\n",
       "0         0      52  False              0   \n",
       "0         1      52   True              0   \n",
       "0         2      52  False              0   \n",
       "0         3      52  False              0   \n",
       "0         4      52  False              0   \n",
       "\n",
       "                                        sentence_lsi  \n",
       "0  [(0, 0.00183121283686), (1, 0.00408547304655),...  \n",
       "0  [(0, 0.154857874989), (1, 0.0239465394874), (2...  \n",
       "0  [(0, 0.118441694899), (1, 0.0393959066023), (2...  \n",
       "0  [(0, 0.359657148598), (1, 0.16882466926), (2, ...  \n",
       "0  [(0, 0.102471883439), (1, 0.0783365336689), (2...  "
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfS['csim'] = final_sent_sims\n",
    "dfS['mean_tfidf'] = stfidf\n",
    "dfS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434741"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find sentiment of each sentence\n",
    "len(sentence_lsi)\n",
    "dfS.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find sentiment of each article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Balancing Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downsample negative sentences so ratio is ~2:1 neg:pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upsample positive sentence with data augmentation\n",
    "\n",
    "# ideas: do they cluster in tf-idf space? try jitter...\n",
    "#        shuffle/swap words from other highlights?\n",
    "#        redistribute among highlights in the same article..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_list_of_lists(lol):\n",
    "    l = []\n",
    "    map(l.extend, lol)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lol = [[1,2,3,4],[12,4,5,2,33],[1],[909,3203,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(flatten_list_of_lists(lol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
