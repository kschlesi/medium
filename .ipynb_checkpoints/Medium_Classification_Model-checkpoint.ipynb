{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training and Testing a Classification Model</h3>\n",
    "\n",
    "In this notebook, I will build a training/test/validation set of sentences from Medium articles. I will label the set and extract features. Then I will train a model and cross-validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from random import randint\n",
    "from sklearn import linear_model, metrics\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step One:</b> Separate out training from test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to postgresql db\n",
    "username = 'kimberly'\n",
    "dbname = 'medium'\n",
    "\n",
    "dbe = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get df, drop missing data\n",
    "df = pd.read_sql('articles', dbe, index_col='postid')\n",
    "df = df.dropna(axis=0,how='any')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Functions to format row of the df, as well as do text processing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions to convert nlikes and ncomments to integer\n",
    "def convert_K(nstr):\n",
    "    spl = nstr.split('K')\n",
    "    if len(spl)==1:\n",
    "        return int(float(spl[0]))\n",
    "    else:\n",
    "        return int(float(spl[0])*1000)\n",
    "    \n",
    "def convert_str(nstr):\n",
    "    nstr = nstr.replace(',','')\n",
    "    if nstr=='':\n",
    "        return None\n",
    "    else:\n",
    "        return int(nstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_paragraph(par,swords):\n",
    "    '''takes one paragraph (string); performs lower, tokenize, remove punctuation/stop words'''\n",
    "    par = par.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(par)\n",
    "    nstop_tokens = [t for t in tokens if (t not in swords and t not in string.punctuation)]\n",
    "    return nstop_tokens                      \n",
    "\n",
    "def process_text_paragraphs(atext,origdb,swords):\n",
    "    '''atext is a list or series of textstrings (one from each article), \n",
    "    origdb is a list or series of corresponding original databse IDs (ints from 0-4)'''\n",
    "    # initial text split\n",
    "    alist = [initial_text_split(a,int(o)) for a,o in zip(atext,origdb)]\n",
    "        \n",
    "    # remove very long articles\n",
    "    removed_articles = [aix for aix,a in enumerate(alist) if len(a)>=250]\n",
    "    alist = [a for a in alist if len(a)<250]\n",
    "    \n",
    "    # process each paragraph\n",
    "    alist = [[process_paragraph(p,swords) for p in a] for a in alist]\n",
    "    \n",
    "    return [alist,removed_articles]\n",
    "\n",
    "def process_text_sentences(atext,origdb,swords):\n",
    "    '''same processing, but does a sentence breakup rather than paragraph'''\n",
    "    # initial text split\n",
    "    alist = [initial_text_split(a,int(o)) for a,o in zip(atext,origdb)]\n",
    "    \n",
    "    # remove very long articles\n",
    "    removed_articles = [aix for aix,a in enumerate(alist) if len(a)>=250]\n",
    "    alist = [a for a in alist if len(a)<250]\n",
    "    \n",
    "    # change paragraph splits to sentence splits\n",
    "    alist = [plist_to_slist(plist) for plist in alist]\n",
    "    \n",
    "    # process each paragraph\n",
    "    alist = [[process_paragraph(p,swords) for p in a] for a in alist]\n",
    "    \n",
    "    return [alist,removed_articles]\n",
    "\n",
    "def initial_text_split(article_text,origdb):\n",
    "    '''takes article text and original db and performs appropriate splitting'''\n",
    "    if origdb in [1,2,3]:\n",
    "        # split into paragraphs\n",
    "        plist = article_text.split('/n')\n",
    "\n",
    "        # remove \\n symbols from within words\n",
    "        plist = [p.replace('\\n','') for p in plist]\n",
    "    \n",
    "    else:\n",
    "        # split into paragraphs\n",
    "        plist = article_text.split('\\n')\n",
    "        \n",
    "    return plist\n",
    "\n",
    "def plist_to_slist(plist):\n",
    "    '''changes a list of paragraphs to a list of sentences'''\n",
    "    spl = [re.split('[/./!/?]',par) for par in plist]\n",
    "    return [s for p in spl for s in p if len(s)>1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now we will use these functions to format the text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define stop word corpus and process text\n",
    "swords = stopwords.words('english')\n",
    "processing_output = process_text_sentences(df.text,df.origdb,swords)\n",
    "ptext = processing_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop too-long articles\n",
    "removed_articles = processing_output[1]\n",
    "dfDrop = df.drop(df.index[removed_articles])\n",
    "#for rem in removed_articles:\n",
    "#    del ptext[rem]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process highlights and titles\n",
    "htext = [plist_to_slist([hilite]) for hilite in dfDrop.highlight]\n",
    "htext = [[process_paragraph(hsent,swords) for hsent in art] for art in htext]\n",
    "ttext = [plist_to_slist([title]) for title in dfDrop.title]\n",
    "ttext = [[process_paragraph(tsent,swords) for tsent in art] for art in ttext]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4640"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(htext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we will LABEL each sentence as 1 or 0 (in highlight or not)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LABEL whether each sentence is in the highlight\n",
    "plabel = []\n",
    "for a,art in enumerate(ptext):\n",
    "    alabel = []\n",
    "    for s in art:\n",
    "        alabel.append(any([(s==hs) for hs in htext[a]]))\n",
    "    plabel.append(alabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457\n",
      "3183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4640"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find articles with no highlight in ptext\n",
    "len(plabel)\n",
    "h_in_ptext = [any(a) for a in plabel]\n",
    "print(len(plabel) - sum(h_in_ptext))\n",
    "print(sum(h_in_ptext))\n",
    "len(plabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that several (about 1/4) of the articles with a highlight do NOT contain the highlight in the p-text (i.e., in the text scraped from p tags in the html).\n",
    "<br><br>\n",
    "These can still be used as negative examples. We will check to see if there is a corresponding positive example, and if not, use the highlight itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get the dataframe together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format rows in dataframe\n",
    "dfDrop.nlikes = dfDrop.nlikes.map(convert_K)\n",
    "dfDrop.ncomments = dfDrop.ncomments.map(convert_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "#print(len(dfDrop.iloc[637].text))\n",
    "#postidToIgnore = dfDrop.index[637]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add processed text to df\n",
    "dfDrop.text = ptext\n",
    "dfDrop.highlight = htext\n",
    "dfDrop.title = ttext\n",
    "dfDrop['label'] = plabel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add article wcount to df\n",
    "wcount = [sum([len(par) for par in art]) for art in ptext]\n",
    "dfDrop['wcount'] = wcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data will include (about) 80% of the articles in the dataframe. \n",
    "First, we separate out a random set of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose test/train sets\n",
    "#test_ix = randint(0,999)\n",
    "#dfTest = dfDrop.iloc([test_ix])\n",
    "#dfTrain = dfDrop.drop(dfDrop.index[test_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Set up training data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4640, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>popdate</th>\n",
       "      <th>url</th>\n",
       "      <th>userid</th>\n",
       "      <th>username</th>\n",
       "      <th>highlight</th>\n",
       "      <th>nlikes</th>\n",
       "      <th>ncomments</th>\n",
       "      <th>ntags</th>\n",
       "      <th>origdb</th>\n",
       "      <th>tags</th>\n",
       "      <th>text</th>\n",
       "      <th>npar</th>\n",
       "      <th>label</th>\n",
       "      <th>wcount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>postid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1015a0f4961d</th>\n",
       "      <td>[[day, one, president, obama, first, family, l...</td>\n",
       "      <td>2016-03-21</td>\n",
       "      <td>https://medium.com/@ObamaWhiteHouse/day-one-pr...</td>\n",
       "      <td>ca9f8f16893b</td>\n",
       "      <td>The Obama White House</td>\n",
       "      <td>[[today, air, force, one, touched, havana, fir...</td>\n",
       "      <td>336</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Cuba,Twitter,Cuba Trip</td>\n",
       "      <td>[[hola, desde, cuba], [today, air, force, one,...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[False, True, False, False, False, False, Fals...</td>\n",
       "      <td>522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101a407e8c61</th>\n",
       "      <td>[[make, makes]]</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>https://medium.com/the-mission/you-dont-make-i...</td>\n",
       "      <td>5ce28105ffbc</td>\n",
       "      <td>Jon Westenberg</td>\n",
       "      <td>[[make, makes]]</td>\n",
       "      <td>549</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Entrepreneurship,Startup,Life</td>\n",
       "      <td>[[always, wanted, make], [grew, dreaming, rock...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030d29376f1</th>\n",
       "      <td>[[ux, infinite, scrolling, vs], [pagination]]</td>\n",
       "      <td>2016-05-02</td>\n",
       "      <td>https://uxplanet.org/ux-infinite-scrolling-vs-...</td>\n",
       "      <td>bcab753a4d4e</td>\n",
       "      <td>Nick Babich</td>\n",
       "      <td>[[instances, infinite, scrolling, effective], ...</td>\n",
       "      <td>1910</td>\n",
       "      <td>46.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>UX,Design,User Experience,UX Design</td>\n",
       "      <td>[[use, infinite, scrolling, pagination, conten...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10315016b299</th>\n",
       "      <td>[[lesson, stereotypes]]</td>\n",
       "      <td>2016-08-20</td>\n",
       "      <td>https://medium.com/@mramsburg85/a-lesson-on-st...</td>\n",
       "      <td>d38709ba4e06</td>\n",
       "      <td>Michael Ramsburg</td>\n",
       "      <td>[[stereotypes, strip, culture, like, mountains...</td>\n",
       "      <td>583</td>\n",
       "      <td>103.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Stereotypes,Appalachia,Culture,Essay,Opinion</td>\n",
       "      <td>[[stereotypes], [mrs], [mitchell, sixth, grade...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10321e751c6d</th>\n",
       "      <td>[[republican, never, trump, means]]</td>\n",
       "      <td>2016-07-30</td>\n",
       "      <td>https://medium.com/@ccmccain/for-this-republic...</td>\n",
       "      <td>4e965facd5f9</td>\n",
       "      <td>Caroline McCain</td>\n",
       "      <td>[[trump, statement, view, unforgivable, speaks...</td>\n",
       "      <td>2500</td>\n",
       "      <td>302.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Hillary Clinton,Donald Trump,Never Trump,2016 ...</td>\n",
       "      <td>[[know, know, woman, fiercely, loyal, friends,...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          title     popdate  \\\n",
       "postid                                                                        \n",
       "1015a0f4961d  [[day, one, president, obama, first, family, l...  2016-03-21   \n",
       "101a407e8c61                                    [[make, makes]]  2016-06-02   \n",
       "1030d29376f1      [[ux, infinite, scrolling, vs], [pagination]]  2016-05-02   \n",
       "10315016b299                            [[lesson, stereotypes]]  2016-08-20   \n",
       "10321e751c6d                [[republican, never, trump, means]]  2016-07-30   \n",
       "\n",
       "                                                            url        userid  \\\n",
       "postid                                                                          \n",
       "1015a0f4961d  https://medium.com/@ObamaWhiteHouse/day-one-pr...  ca9f8f16893b   \n",
       "101a407e8c61  https://medium.com/the-mission/you-dont-make-i...  5ce28105ffbc   \n",
       "1030d29376f1  https://uxplanet.org/ux-infinite-scrolling-vs-...  bcab753a4d4e   \n",
       "10315016b299  https://medium.com/@mramsburg85/a-lesson-on-st...  d38709ba4e06   \n",
       "10321e751c6d  https://medium.com/@ccmccain/for-this-republic...  4e965facd5f9   \n",
       "\n",
       "                           username  \\\n",
       "postid                                \n",
       "1015a0f4961d  The Obama White House   \n",
       "101a407e8c61         Jon Westenberg   \n",
       "1030d29376f1            Nick Babich   \n",
       "10315016b299       Michael Ramsburg   \n",
       "10321e751c6d        Caroline McCain   \n",
       "\n",
       "                                                      highlight  nlikes  \\\n",
       "postid                                                                    \n",
       "1015a0f4961d  [[today, air, force, one, touched, havana, fir...     336   \n",
       "101a407e8c61                                    [[make, makes]]     549   \n",
       "1030d29376f1  [[instances, infinite, scrolling, effective], ...    1910   \n",
       "10315016b299  [[stereotypes, strip, culture, like, mountains...     583   \n",
       "10321e751c6d  [[trump, statement, view, unforgivable, speaks...    2500   \n",
       "\n",
       "              ncomments  ntags  origdb  \\\n",
       "postid                                   \n",
       "1015a0f4961d       15.0    3.0     3.0   \n",
       "101a407e8c61       37.0    3.0     3.0   \n",
       "1030d29376f1       46.0    4.0     3.0   \n",
       "10315016b299      103.0    5.0     3.0   \n",
       "10321e751c6d      302.0    5.0     3.0   \n",
       "\n",
       "                                                           tags  \\\n",
       "postid                                                            \n",
       "1015a0f4961d                             Cuba,Twitter,Cuba Trip   \n",
       "101a407e8c61                      Entrepreneurship,Startup,Life   \n",
       "1030d29376f1                UX,Design,User Experience,UX Design   \n",
       "10315016b299       Stereotypes,Appalachia,Culture,Essay,Opinion   \n",
       "10321e751c6d  Hillary Clinton,Donald Trump,Never Trump,2016 ...   \n",
       "\n",
       "                                                           text  npar  \\\n",
       "postid                                                                  \n",
       "1015a0f4961d  [[hola, desde, cuba], [today, air, force, one,...  20.0   \n",
       "101a407e8c61  [[always, wanted, make], [grew, dreaming, rock...  21.0   \n",
       "1030d29376f1  [[use, infinite, scrolling, pagination, conten...  34.0   \n",
       "10315016b299  [[stereotypes], [mrs], [mitchell, sixth, grade...  12.0   \n",
       "10321e751c6d  [[know, know, woman, fiercely, loyal, friends,...  45.0   \n",
       "\n",
       "                                                          label  wcount  \n",
       "postid                                                                   \n",
       "1015a0f4961d  [False, True, False, False, False, False, Fals...     522  \n",
       "101a407e8c61  [False, False, False, False, False, False, Fal...     393  \n",
       "1030d29376f1  [False, False, False, False, False, False, Fal...     850  \n",
       "10315016b299  [False, False, False, False, False, False, Fal...     381  \n",
       "10321e751c6d  [False, False, False, False, False, False, Fal...     993  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dfDrop.shape)\n",
    "dfDrop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sentence-wise split:</b> We set up the data by sentence..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up a dataframe for sentences...\n",
    "dfS = pd.DataFrame()\n",
    "\n",
    "for art in dfDrop.index\n",
    "    slist = dfDrop.text[art]\n",
    "    for nsent in range(len(slist)):\n",
    "        sent = slist[nsent]\n",
    "        dfRow = pd.DataFrame([art,sent,len(sent),nsent,len(slist),dfDrop.label[art][nsent]])\n",
    "        dfRow = dfRow.T\n",
    "        dfRow.columns = ['postid','sentence','swcount','sposition','alength','slabel']\n",
    "        dfS = pd.concat([dfS,dfRow])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434741, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>postid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>swcount</th>\n",
       "      <th>sposition</th>\n",
       "      <th>alength</th>\n",
       "      <th>slabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[hola, desde, cuba]</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[today, air, force, one, touched, havana, firs...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[question, remarkable, moment, relationship, u...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[also, landmark, progress, made, since, presid...</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1015a0f4961d</td>\n",
       "      <td>[trip, also, professionally, personally, meani...</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         postid                                           sentence swcount  \\\n",
       "0  1015a0f4961d                                [hola, desde, cuba]       3   \n",
       "0  1015a0f4961d  [today, air, force, one, touched, havana, firs...       9   \n",
       "0  1015a0f4961d  [question, remarkable, moment, relationship, u...       9   \n",
       "0  1015a0f4961d  [also, landmark, progress, made, since, presid...      29   \n",
       "0  1015a0f4961d  [trip, also, professionally, personally, meani...      19   \n",
       "\n",
       "  sposition alength slabel  \n",
       "0         0      52  False  \n",
       "0         1      52   True  \n",
       "0         2      52  False  \n",
       "0         3      52  False  \n",
       "0         4      52  False  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dfS.shape)\n",
    "dfS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfS.to_sql('sentences',dbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1457, 15)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, add sentence entries for the highlights not in ptext.\n",
    "\n",
    "h_in_ptext = [any(a) for a in plabel]\n",
    "dfNotInP = dfDrop.drop(dfDrop.index[h_in_ptext])\n",
    "dfNotInP.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dfComb = dfS.copy()\n",
    "\n",
    "# for art in dfNotInP.index:\n",
    "#     slist = dfNotInP.text[art]\n",
    "#     for nsent in range(len(slist)):\n",
    "#         sent = slist[nsent]\n",
    "#         dfRow = pd.DataFrame([art,sent,len(sent),nsent,len(slist),dfNotInP.label[art][nsent]])\n",
    "#         dfRow = dfRow.T\n",
    "#         dfRow.columns = ['postid','sentence','swcount','sposition','alength','slabel']\n",
    "#         dfComb = pd.concat([dfComb,dfRow])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfComb.to_sql('sentences_train_addnotinp',dbe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(564102, 6)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfComb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adding article data:</b> We perform a merge to add article-wise data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434741, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "postid       False\n",
       "sentence     False\n",
       "swcount      False\n",
       "sposition    False\n",
       "alength      False\n",
       "slabel       False\n",
       "title        False\n",
       "nlikes       False\n",
       "npar         False\n",
       "wcount       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# left merge dfS -> dfTrain on postid\n",
    "\n",
    "dfX = pd.merge(dfS, dfTrain[['title','nlikes','npar','wcount']], \n",
    "               how='left', left_on='postid', left_index=False, right_index=True, sort=False)\n",
    "print(dfX.shape)\n",
    "nsamp = dfX.shape[0]\n",
    "dfX.isnull().any()\n",
    "#dfX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the 'ncomments' column has a bunch of NaNs, so here we have dropped it. Later we will investigate why (is it the dumb `convert_str()` function?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434741"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfX = dfX.dropna(how='any')\n",
    "dfX.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Now, we set up the model itself.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scikitlearn logistic regression... fit (with 2/3 of X, y from above)\n",
    "\n",
    "dfY = dfX['slabel']\n",
    "dfX = dfX[['swcount','sposition','alength','nlikes','wcount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(289827,)\n",
      "(289827, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spl = math.floor(2*nsamp/3)\n",
    "ytrain = dfY.iloc[0:spl].astype(int)\n",
    "Xtrain = dfX.iloc[0:spl]\n",
    "ytest = dfY.iloc[spl:].astype(int)\n",
    "Xtest = dfX.iloc[spl:]\n",
    "print(ytrain.shape)\n",
    "print(Xtrain.shape)\n",
    "lrm = linear_model.LogisticRegression()\n",
    "lrm.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98661270344 0.98661270344\n",
      "0.985922685179 0.985922685179\n"
     ]
    }
   ],
   "source": [
    "# test on the 2/3 of training set\n",
    "print(lrm.score(Xtrain,ytrain), 1 - ytrain.mean())\n",
    "\n",
    "# test on the other 1/3 of X, y\n",
    "print(lrm.score(Xtest,ytest), 1 - ytest.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is predicting \"no highlight\" for every sample. This is what I expected, given the extremely unbalanced nature of the data. I will try balancing techniques (just as soon as I get Flask to work...)\n",
    "<br><br>\n",
    "Okay. Flask works. Now I need a predicting model.\n",
    "<br><br>\n",
    "<ul>\n",
    "<li>The very first thing to do is to add the other positive examples I am missing. Find the instances of highlights not included in the ptext adn add them to the dfS. <i><b>Done above, resulting in dfComb</b></i></li>\n",
    "<li>THEN add the new dfS to the databse!!</li>\n",
    "<li>The next thing, I think, is to try the tf-idf embedding via <b>gensim</b>. Cluster the words based on term frequency v. inverse article frequency, then choose clusters that correspond the the first few hundred eigenvectors of the basis.</li>\n",
    "<li>Once you have done that, </li>\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make ROC curve to compare thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Adding New Features</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is getting the text by article and making tf-idf vectors.\n",
    "<ol>\n",
    "<li>Text by article is created by joining sentences. </li>\n",
    "<li>Frequency count of each word in the whole corpus. </li>\n",
    "<li>Remove words that appear only once. </li>\n",
    "<li>Create a dictionary and a corpus. </li>\n",
    "<li></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text by article\n",
    "def to_atext(art):\n",
    "    ntext = []\n",
    "    for s in art:\n",
    "        ntext.extend(s)\n",
    "    return ntext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'desde', 'cuba', 'today', 'air', 'force', 'one', 'touched', 'havana', 'first', 'time', 'history', 'question', 'remarkable', 'moment', 'relationship', 'united', 'states', 'cuba', 'governments', 'people', 'also', 'landmark', 'progress', 'made', 'since', 'president', 'obama', 'decided', 'reform', 'failed', 'cold', 'war', 'era', 'policies', 'past', 'chart', 'new', 'course', 'would', 'actually', 'advance', 'american', 'interests', 'values', 'help', 'cuban', 'people', 'improve', 'lives', 'trip', 'also', 'professionally', 'personally', 'meaningful', 'special', 'assistant', 'advisor', 'antoinette', 'rangel', 'cuban', 'american', 'learned', 'country', 'stories', 'abuela', 'maria', 'shared', 'growing', 'good', 'illustration', 'closely', 'two', 'countries', 'linked', 'trip', 'potential', 'change', 'lives', 'families', 'cuba', 'united', 'states', 'looking', 'forward', 'learning', 'first', 'hand', 'cuban', 'culture', 'life', 'bringing', 'white', 'house', 'press', 'corps', 'along', 'ride', 'daily', 'basis', 'answering', 'questions', 'president', 'trip', 'cuba', 'pose', 'question', 'twitter', 'using', 'askpresssec', 'answer', 'presssec', 'ground', 'let', 'start', 'questions', 'know', 'many', 'americans', 'cuba', 'visit', 'q', 'president', 'decide', 'change', 'u', 'policy', 'toward', 'cuba', 'fifty', 'years', 'united', 'states', 'wedded', 'policy', 'isolate', 'pressure', 'cuba', 'without', 'seeing', 'results', 'cuba', 'political', 'system', 'change', 'making', 'life', 'better', 'cuban', 'people', 'citing', 'lack', 'progress', 'december', '17', '2014', 'president', 'announced', 'country', 'policy', 'toward', 'cuba', 'changing', 'course', 'since', 'made', 'substantial', 'progress', 'normalizing', 'relations', 'pleased', 'see', 'cuban', 'people', 'overwhelmingly', 'support', 'new', 'policy', 'fact', 'democrats', 'republicans', 'congress', 'joining', 'president', 'trip', 'good', 'illustration', 'strong', 'bipartisan', 'support', 'new', 'opening', 'cuba', 'opened', 'embassy', 'havana', '50', 'years', 'shuttered', 'doors', 'expanded', 'commercial', 'ties', 'made', 'easier', 'americans', 'travel', 'business', 'restored', 'direct', 'flights', 'direct', 'mail', 'case', 'missed', 'check', 'president', 'obama', 'letter', '76', 'year', 'old', 'woman', 'cuba', 'carried', 'first', 'mail', 'flight', '50', 'years', 'significant', 'changes', 'welcome', 'start', 'important', 'mission', 'expanding', 'people', 'people', 'interaction', 'commercial', 'enterprise', 'today', 'americans', 'visiting', 'cuba', 'time', 'last', '50', 'years', 'cuban', 'american', 'families', 'american', 'students', 'volunteers', 'faith', 'leaders', 'entrepreneurs', 'president', 'trip', 'big', 'opportunity', 'advance', 'progress', 'beginning', 'new', 'phase', 'progress', 'reflects', 'interests', 'values', 'better', 'future', 'cuban', 'people', 'take', 'time', 'path', 'president', 'decided', 'time', 'take', 'necessary', 'steps', 'toward', 'better', 'future', 'citizens', 'countries', 'q', 'cuba', 'short', 'trip', 'lot', 'get', 'done', 'colleague', 'ben', 'rhodes', 'president', 'deputy', 'national', 'security', 'advisor', 'along', 'help', 'senior', 'advisor', 'bernadette', 'meehan', 'taking', 'lead', 'implementing', 'policy', 'negotiating', 'cuban', 'government', 'helpfully', 'laid', 'marquee', 'events', 'president', 'scheduled', 'highlights', 'sunday', 'taking', 'walking', 'tour', 'old', 'havana', 'met', 'carinal', 'ortega', 'latin', 'rite', 'archbishop', 'archdiocese', 'havana', 'cardinal', 'catholic', 'church', 'tour', 'places', 'illustrate', 'history', 'cultural', 'significance', 'beautiful', 'city', 'monday', 'lay', 'wreath', 'jose', 'marti', 'memorial', 'participate', 'discussion', 'entrepreneurship', 'opportunity', 'cuentapropistas', 'cuban', 'entrepreneurs', 'also', 'head', 'revolutionary', 'palace', 'meet', 'president', 'raúl', 'castro', 'discuss', 'together', 'make', 'easier', 'trade', 'easier', 'cubans', 'access', 'internet', 'start', 'businesses', 'tuesday', 'first', 'family', 'attend', 'baseball', 'game', 'tampa', 'bay', 'rays', 'cuban', 'national', 'team', 'latinoamerican', 'stadium', 'first', 'time', 'mlb', 'team', 'played', 'cuba', 'since', '1999', 'tampa', 'bay', 'rays', 'lottery', 'pick', 'among', 'teams', 'wanted', 'come', 'play', 'one', 'things', 'sure', 'royals', 'come', 'next', 'coming', 'q', 'president', 'hope', 'see', 'happen', 'cuba', 'visit', 'president', 'change', 'policy', 'historic', 'visit', 'come', 'belief', 'u', 'help', 'make', 'difference', 'cuban', 'people', 'changing', 'way', 'americans', 'engage', 'cubans', 'foster', 'hope', 'future', 'making', 'president', 'policies', 'geared', 'toward', 'providing', 'opportunity', 'cuban', 'people', 'rather', 'isolating', 'past', 'course', 'real', 'differences', 'political', 'economic', 'systems', 'difficult', 'histories', 'prevented', 'progress', 'historic', 'trip', 'president', 'hopes', 'share', 'vision', 'americans', 'cubans', 'together', 'ensure', 'future', 'cuba', 'reflects', 'freedom', 'opportunity', 'people', 'good', 'cuba', 'good', 'america', 'especially', 'millions', 'americans', 'deep', 'enduring', 'ties', 'cuba', 'follow', 'along', 'follow', 'presssec', 'twitter', 'explore', 'cuba', 'future', 'build', 'together', 'trip', 'remember', 'account', 'maintained', 'national', 'archives', 'records', 'administration', 'nara', 'serve', 'archive', 'obama', 'administration', 'content']\n"
     ]
    }
   ],
   "source": [
    "# flatten second row\n",
    "atext = [to_atext(art) for art in ptext]\n",
    "print(atext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do stemming with porter stemmer\n",
    "porter = PorterStemmer()\n",
    "atext = [[porter.stem(t) for t in art] for art in atext]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n"
     ]
    }
   ],
   "source": [
    "htext = [[[porter.stem(w) for w in s] for s in hil] for hil in htext]\n",
    "ttext = [[[porter.stem(w) for w in s] for s in tit] for tit in ttext]\n",
    "print(htext[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have flattened the text within each document and stemmed each word, we will go on to frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gensim #(above...)\n",
    "\n",
    "# find frequency of each word in corpus (article list)\n",
    "frequency = defaultdict(int)\n",
    "for subtext in atext:\n",
    "    for token in subtext:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove words/stems with frequency of 1\n",
    "atext = [[token for token in art if frequency[token] > 1]\n",
    "         for art in atext]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a <b>dictionary</b>, which is a word-frequency mapping space of n (= number of words) dimensions. It spans the bag-of-words space of our document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we create a dictionary\n",
    "dictionary = corpora.Dictionary(atext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n",
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n"
     ]
    }
   ],
   "source": [
    "# practice representing a highlight in this dictionary space\n",
    "test_doc = htext[0][0]\n",
    "print(test_doc)\n",
    "new_vec = dictionary.doc2bow(test_doc)\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a corpus\n",
    "output = open('atext.pkl', 'wb')\n",
    "pickle.dump(atext, output)\n",
    "output.close()\n",
    "\n",
    "o2 = open('dicty.pkl', 'wb')\n",
    "pickle.dump(dictionary, o2)\n",
    "o2.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the corpus elsewhere\n",
    "# read in\n",
    "from gensim_corpus import get_corpus\n",
    "#corp = get_corpus('atext_train_corp.mm')\n",
    "corp = corpora.MmCorpus('atext_train_corp.mm')\n",
    "#print(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n",
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "[(2, 0.1217169239582691), (3, 0.30230312910390755), (4, 0.1828764398150676), (5, 0.017012819573171706), (6, 0.2557849334233673), (7, 0.8665596984748893), (8, 0.05368257118654773), (9, 0.019686602255689127), (10, 0.20110583368465484)]\n"
     ]
    }
   ],
   "source": [
    "# create tfidf model\n",
    "tfidf = models.TfidfModel(corp)\n",
    "corp_tfidf = tfidf[corp]\n",
    "print(htext[0][0])\n",
    "print(new_vec) # dictionary space\n",
    "print(tfidf[new_vec]) # tf-idf space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a tf-idf model, we can rank each sentence by the importance of its words. Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hola', 'desde', 'cuba']"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptext[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'desde', 'cuba']\n",
      "[(1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# for each sentence, calculate the tf-idf score (average over sentence words)\n",
    "sentence_corp = []\n",
    "for art in ptext:\n",
    "    for sent in art:\n",
    "        sentence_corp.append( dictionary.doc2bow(sent) )\n",
    "\n",
    "print(ptext[0][0])\n",
    "print(sentence_corp[0])\n",
    "sentence_tfidf = tfidf[sentence_corp]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/anaconda3/envs/insight_env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/kimberly/anaconda3/envs/insight_env/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# stfidf feature\n",
    "stfidf = []\n",
    "for sent_rep in sentence_tfidf:\n",
    "    stfidf.append( np.mean([wr[1] for wr in sent_rep]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434792"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.argwhere(~np.isnan(stfidf)))\n",
    "len(stfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3031c6e8fd1d\n",
      "434741\n",
      "434741\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "# make a vector with the nans for the dropped article\n",
    "print(postidToIgnore)\n",
    "\n",
    "# splx = np.cumsum([len(art) for ix,art in enumerate(ptext) if ix<637])[-1]\n",
    "\n",
    "# tmp = [s for ix,s in enumerate(stfidf) if ix<splx]\n",
    "# tmp.extend([None]*len(ptext[637]))\n",
    "# tmp.extend([s for ix,s in enumerate(stfidf) if ix>splx])\n",
    "# stfidf = tmp\n",
    "\n",
    "tmp = []\n",
    "for ax in range(len(ptext)):\n",
    "    if ax==637:\n",
    "        tmp.extend([])\n",
    "    else:\n",
    "        cpy = copy.deepcopy(stfidf[0:len(ptext[ax])])\n",
    "        tmp.extend(cpy)\n",
    "        stfidf = stfidf[len(art):]\n",
    "        \n",
    "        \n",
    "\n",
    "print(len(tmp))\n",
    "stfidf = tmp\n",
    "print(len(stfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>THERE IS A NEW FEATURE, BOOYA</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3X28HVV97/HPlwRDBAlPMRcTIAhRBFQwEbGlvShagk9Q\nBQxFiRhJFSrY1lZQW6m9sVBbUaqg+ERAKkQEiRTkYhBRMWCiQAhIiQRMYoAQQsLDhZLwvX/MOmZn\nu885E7L3Odk53/frNa+zZu1Za9YM4fzOWmv2GtkmIiKiHbYa7AZERMSWI0ElIiLaJkElIiLaJkEl\nIiLaJkElIiLaJkElIiLaJkElBoWkP5e0RNITkg5s8fnHJX2tj/L3S3pTSUvSNyWtknRrjXOPl2RJ\nw8v+tZKmbsr1REQlQSU6rjEANPg34K9sb2f7V81lbH/G9gdqnuIQ4M3AONsHbWz7bB9he2Z/x5VA\ntPfG1t+tJL1P0k8Hux3RXRJUYrDsASxsY133236yTfVt1iQNG+w2RPQmQSU6StLFwO7A98tQ18ck\nPQEMA26X9Jteyp0p6VsN+++V9ICklZI+0ZA/Dfga8PpS/z+1qGuYpH+T9Iik+4C3Nn1+o6QPlPTe\nkn4saXU5/rKSf1M5/PZynne3OM9Wkj5Z2vmwpIskjSqfXSvpr5qOv13SO0t6H0nXS3pU0j2Sjm04\n7kJJ50u6RtKTwBtanPt9ku6T9LikxZKOb/js/ZLuLsOD10nao+EzS/qgpHslPSbpS2U48RXAlxvu\n62Pl+BHlXv5W0kOSvixpZPnsUElLJf1tuf7lkk5sONdISf9e7s9qST9tKHuwpJtLG26XdGida4vN\nkO1s2Tq6AfcDb2rKM7B3H2XOBL5V0vsCTwB/CowAPges7akTeB/w0z7q+iDwa2A3YCfgR+X8w8vn\nNwIfKOlvA5+g+oNrG+CQjWjz+4FFwEuB7YArgIvLZycAP2s4dl/gsXI92wJLgBOB4cCBwCPAvuXY\nC4HVwB/3tKvpvNsCa4CXl/1dgf1K+sjSpleUuj8J3Nx0TVcDO1AF/xXA5N7uK3AOMLvcxxcB3wf+\npXx2aPnv8mlga+AtwFPAjuXzL5V7PZbqj4o/Ktc/FlhZjt+KaihzJTC6r2vLtnlu6alENzgauNr2\nTbafAf4BeG4jyh8LfN72EtuPAv/Sx7HPUg2nvcT207Y3Zk7heOBztu+z/QRwBjClPBBwJXBAQy/h\neOCKcj1voxq++6btta7mmL4LHNNQ91W2f2b7OdtPtzj3c8D+kkbaXm67Z2jxg1S/9O+2vRb4TFM7\nAM6y/Zjt31IF3ANaXZwkAdOBv7b9qO3HS31TGg57Fvi07WdtX0P1x8DLJW1FFXRPs73M9jrbN5fr\nfw9wje1ryvVdD8yjCjJ9XVtshhJUYtBJOr4MsTwh6doWh7yE6i95AFzNnazciFNsUB54oI9j/x4Q\ncKukhZLev5Hnaaz7AarewZjyC/i/WP8L+DjgkpLeA3hdGfp5rAw1HQ/8r4a6Gtu/gXI/3k0VQJZL\n+i9J+zTU/YWGeh8t1ze2oYoHG9JPUfWyWhkNvBCY31DfD0p+j5UleDXXtwtVz6/VcOcewDFN138I\nsGs/1xaboQSVGAh9LoVt+xJXT4FtZ/uIFocspxq6AkDSC4GdN+L8G5SnGubprS0P2j7J9kuAvwTO\nU/0nvn5H9Quy8TxrgYfK/reB4yS9nuoX7I9K/hLgx7Z3aNi2s/2hxqb1dWLb19l+M9Xw0K+BrzbU\n/ZdNdY+0fXON62k+5yPA/6Mafuqpa5Tt3oJQc9mngb1afLaEapiwsY3b2j6rn2uLzVCCSgyEh6jm\nGZ6vy4G3STpE0guoxuw35t/uLOBUSeMk7Qic3tuBko6RNK7srqL6xdoz1NbfdXwb+GtJe0rajmpo\n6LKGv9yvoQo6ny75PfVeDbxM1cMIW5fttWWyvF+Sxkg6UtK2wDNUQ049dX8ZOEPSfuXYUZKO6aWq\nZg8B48o9p7T3q8A5kl5c6hsr6fD+KiplvwF8TtJLVD088XpJI4BvAW+XdHjJ36ZM+o/r59piM5Sg\nEgPhX4BPlqGNj25s4TKGfgrwn1S9jlXA0o2o4qvAdcDtwC+pJtB781rgFlVPqM2mmgO4r3x2JjCz\nXMexLcp+A7gYuAlYTPWX+YcbruOZcu43lWvpyX8c+DOqobHfUQ1HnU01iV3HVsDflLKPAv8b+FCp\n+8pS16WS1gB3Aq16g63cQPXY94OSHil5H6Oa+J9b6vsh8PKa9X0UWAD8orTzbGAr20uoHij4ONWD\nAkuAvyvX1eu1xeZJdl7SFRER7ZGeSkREtE2CSkREtE2CSkREtE2CSkREtM3wwW7AQNtll108fvz4\nwW5GRERXmT9//iO2R/d33JALKuPHj2fevHmD3YyIiK4iqa+VKH4vw18REdE2CSoREdE2CSoREdE2\nCSoREdE2CSoREdE2CSoREdE2CSoREdE2CSoREdE2CSoREdE2Q+4b9RERW7pXznxly/wFUxd0/Nzp\nqURERNskqERERNskqERERNskqERERNskqERERNskqERERNskqERERNskqERERNskqERERNskqERE\nRNt0LKhIermk2xq2NZI+ImknSddLurf83LGhzBmSFkm6R9LhDfkTJS0on50rSSV/hKTLSv4tksZ3\n6noiIqJ/HQsqtu+xfYDtA4CJwFPAlcDpwBzbE4A5ZR9J+wJTgP2AycB5koaV6s4HTgImlG1yyZ8G\nrLK9N3AOcHanriciIvo3UMNfhwG/sf0AcCQws+TPBI4q6SOBS20/Y3sxsAg4SNKuwPa259o2cFFT\nmZ66LgcO6+nFRETEwBuooDIF+HZJj7G9vKQfBMaU9FhgSUOZpSVvbEk3529QxvZaYDWwc/PJJU2X\nNE/SvBUrVmz61UREREsdDyqSXgC8A/hO82el5+FOt8H2BbYn2Z40evToTp8uImLIGoieyhHAL20/\nVPYfKkNalJ8Pl/xlwG4N5caVvGUl3Zy/QRlJw4FRwMoOXENERNQwEEHlONYPfQHMBqaW9FTgqob8\nKeWJrj2pJuRvLUNlayQdXOZLTmgq01PX0cANpfcTERGDoKNvfpS0LfBm4C8bss8CZkmaBjwAHAtg\ne6GkWcBdwFrgFNvrSpmTgQuBkcC1ZQP4OnCxpEXAo1RzNxERMUg6GlRsP0nTxLntlVRPg7U6fgYw\no0X+PGD/FvlPA8e0pbEREbHJ8o36iIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIhomwSV\niIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIho\nmwSViIhomwSViIhom44GFUk7SLpc0q8l3S3p9ZJ2knS9pHvLzx0bjj9D0iJJ90g6vCF/oqQF5bNz\nJankj5B0Wcm/RdL4Tl5PRET0rdM9lS8AP7C9D/Bq4G7gdGCO7QnAnLKPpH2BKcB+wGTgPEnDSj3n\nAycBE8o2ueRPA1bZ3hs4Bzi7w9cTERF96FhQkTQK+FPg6wC2/8f2Y8CRwMxy2EzgqJI+ErjU9jO2\nFwOLgIMk7Qpsb3uubQMXNZXpqety4LCeXkxERAy8TvZU9gRWAN+U9CtJX5O0LTDG9vJyzIPAmJIe\nCyxpKL+05I0t6eb8DcrYXgusBnZuboik6ZLmSZq3YsWKtlxcRET8oU4GleHAa4DzbR8IPEkZ6upR\neh7uYBt6znOB7Um2J40ePbrTp4uIGLI6GVSWAktt31L2L6cKMg+VIS3Kz4fL58uA3RrKjyt5y0q6\nOX+DMpKGA6OAlW2/koiIqKVjQcX2g8ASSS8vWYcBdwGzgaklbypwVUnPBqaUJ7r2pJqQv7UMla2R\ndHCZLzmhqUxPXUcDN5TeT0REDILhHa7/w8Alkl4A3AecSBXIZkmaBjwAHAtge6GkWVSBZy1wiu11\npZ6TgQuBkcC1ZYPqIYCLJS0CHqV6eiwiIgZJR4OK7duASS0+OqyX42cAM1rkzwP2b5H/NHDMJjYz\nIiLaZKOGvyTtKOlVnWpMRER0t36DiqQbJW0vaSfgl8BXJX2u802LiIhuU6enMsr2GuCdwEW2Xwe8\nqbPNioiIblQnqAwvj/4eC1zd4fZEREQXqxNUPg1cB/zG9i8kvRS4t7PNioiIbtTv01+2vwN8p2H/\nPuBdnWxURER0pzoT9S+TNEfSnWX/VZI+2fmmRUREt6kz/PVV4AzgWQDbd5AvGUZERAt1gsoLbd/a\nlLe2E42JiIjuVieoPCJpL8pqwpKOBpb3XSQiIoaiOsu0nAJcAOwjaRmwGHhPR1sVERFdqc7TX/cB\nbyov2NrK9uOdb1ZERHSjOk9/fUbSDraftP14Wf/r/wxE4yIiorvUmVM5orxbHgDbq4C3dK5JERHR\nreoElWGSRvTsSBoJjOjj+IiIGKLqTNRfAsyR9M2yfyIws3NNioiIblVnov5sSXew/sVa/2z7us42\nKyIiulGtNz/abnyFb0REREt1nv56p6R7Ja2WtEbS45LWDETjIiKiu9SZqP9X4B22R9ne3vaLbG9f\np3JJ90taIOk2SfNK3k6Sri+B6npJOzYcf4akRZLukXR4Q/7EUs8iSedKUskfIemykn+LpPEbc/ER\nEdFedYLKQ7bv3oRzvMH2AbYnlf3TgTm2JwBzyj6S9qVaqHI/YDJwnqRhpcz5wEnAhLJNLvnTgFW2\n9wbOAc7ehHZGRMQmqhNU5pXewHFlKOydkt65Cec8kvVPj80EjmrIv9T2M7YXA4uAg8pbJ7e3Pde2\ngYuayvTUdTlwWE8vJiIiBl6difrtgaeAP2vIM3BFjbIGfihpHfAV2xcAY2z3LEj5IDCmpMcCcxvK\nLi15z5Z0c35PmSUAttdKWg3sDDzS2AhJ04HpALvvvnuNZkdExPNR55HiEzeh/kNsL5P0YuB6Sb9u\nqtuSvAn111KC2QUAkyZN6vj5IiKGqo6++dH2svLzYeBK4CDgoTKkRfn5cDl8GbBbQ/FxJW9ZSTfn\nb1BG0nBgFLCyTtsiIqL9OvbmR0nbSnpRT5pq+OxOYDYwtRw2FbiqpGcDU8oTXXtSTcjfWobK1kg6\nuMyXnNBUpqeuo4EbyrxLREQMgjpzKi+0fWvT/HedNz+OAa4s5YYD/2n7B5J+AcySNA14ADgWwPZC\nSbOAu0r9p9heV+o6GbgQGEn1JcyeL2J+HbhY0iLgUfKa44iIQVUnqDyvNz+W97C8ukX+StYv+dL8\n2QxgRov8ecD+LfKfBo7pry0RETEwnu+bH4/vaKsiIqIr1Qkqtr3Bmx/LnEdERMQG6kzUfxeg582P\nJe/yzjUpIiK6Va89FUn7UC2ZMqrpG/TbA9t0umEREdF9+hr+ejnwNmAH4O0N+Y9TrcMVERGxgV6D\niu2rgKskvd72zwewTRER0aXqTNQvkvRxYHzj8bbf36lGRUREd6oTVK4CfgL8EFjXz7ERETGE1f1G\n/cc63pKIiOh6dR4pvlrSWzrekoiI6Hp1gsppVIHl6byjPiIi+lLnfSovGoiGRERE96vzPhVJeo+k\nfyj7u0k6qPNNi4iIblNn+Os84PXAX5T9J4AvdaxFERHRteo8/fU626+R9CsA26skvaDD7YqIiC5U\np6fyrKRhrH+fymjguY62KiIiulKdoHIu1fvlXyxpBvBT4DMdbVVERHSlOk9/XSJpPtXbGgUcZfvu\njrcsIiK6Tp2nv/YCFtv+EnAn8GZJO9Q9gaRhkn4l6eqyv5Ok6yXdW37u2HDsGZIWSbpH0uEN+RMl\nLSifnavy4ntJIyRdVvJvkTS+9pVHRETb1X1J1zpJewNfAXYD/nMjznEa0NizOR2YY3sCMKfsI2lf\nYArVO1wmA+eVuRyA86mW259QtsklfxqwyvbewDnA2RvRroiIaLM6QeU522uBdwJftP13wK51Kpc0\nDngr8LWG7COBmSU9EziqIf9S28/YXgwsAg6StCuwve25tg1c1FSmp67LgcN6ejERETHw6j79dRxw\nAnB1ydu6Zv2fB/6eDZ8WG2N7eUk/CIwp6bHAkobjlpa8sSXdnL9BmRL4VgM712xbRES0WZ2gciLV\nlx9n2F4saU/g4v4KSXob8LDt+b0dU3oertvY50vSdEnzJM1bsWJFp08XETFk1Xn66y7g1Ib9xdSb\nu/hj4B1lheNtgO0lfQt4SNKutpeXoa2Hy/HLqOZreowrectKujm/scxSScOBUcDKFtdwAXABwKRJ\nkzoexCIihqo6PZXnxfYZtsfZHk81AX+D7fcAs4Gp5bCpVC8Bo+RPKU907Uk1IX9rGSpbI+ngMl9y\nQlOZnrqOLudI0IiIGCR1lmlpt7OAWZKmAQ8AxwLYXihpFnAXsBY4xXbPmyZPBi4ERgLXlg3g68DF\nkhYBj1IFr4iIGCS9BhVJF9t+r6TTbH9hU05i+0bgxpJeSfVFylbHzQBmtMifB+zfIv9p4JhNaVtE\nRLRPX8NfEyW9BHi/pB3LlxZ/vw1UAyMionv0Nfz1ZaovJ74UmE+1REsPl/yIiIjf67WnYvtc268A\nvmH7pbb3bNgSUCIi4g/UeaT4Q5JeDfxJybrJ9h2dbVZERHSjOgtKngpcAry4bJdI+nCnGxYREd2n\nziPFH6B6++OTAJLOBn4O/EcnGxYREd2nzpcfBaxr2F/HhpP2ERERQL2eyjeBWyRdWfaPovrSYURE\nxAbqTNR/TtKNwCEl60Tbv+poqyIioivVWqbF9i+BX3a4LRER0eU6tqBkREQMPQkqERHRNn0GFUnD\nJP1ooBoTERHdrc+gUpaef07SqAFqT0REdLE6E/VPAAskXQ882ZNp+9Tei0RExFBUJ6hcUbaIiIg+\n1fmeykxJI4Hdbd8zAG2KiIguVWdBybcDtwE/KPsHSJrd6YZFRET3qfNI8ZnAQcBjALZvIy/oioiI\nFuoElWdtr27Ke66/QpK2kXSrpNslLZT0TyV/J0nXS7q3/NyxocwZkhZJukfS4Q35EyUtKJ+dK0kl\nf4Sky0r+LZLG17noiIjojDpBZaGkvwCGSZog6T+Am2uUewZ4o+1XAwcAkyUdDJwOzLE9gep1xacD\nSNoXmALsB0wGzpM0rNR1PnASMKFsk0v+NGCV7b2Bc4Cza7QrIiI6pE5Q+TDVL/pngG8Da4CP9FfI\nlSfK7tZlM3AkMLPkz6Ra9ZiSf6ntZ2wvBhYBB0naFdje9lzbBi5qKtNT1+XAYT29mIiIGHh1nv56\nCvhEeTmXbT9et/LS05gP7A18yfYtksbYXl4OeRAYU9JjgbkNxZeWvGdLujm/p8yS0s61klYDOwOP\nNLVjOjAdYPfdd6/b/IiI2Eh1nv56raQFwB1UX4K8XdLEOpXbXmf7AGAcVa9j/6bPTdV76SjbF9ie\nZHvS6NGjO326iIghq87w19eBk22Ptz0eOIXqxV212X4M+BHVXMhDZUiL8vPhctgyYLeGYuNK3rKS\nbs7foIyk4cAoYOXGtC0iItqnTlBZZ/snPTu2fwqs7a+QpNGSdijpkcCbgV8Ds4Gp5bCpwFUlPRuY\nUp7o2pNqQv7WMlS2RtLBZb7khKYyPXUdDdxQej8RETEIep1TkfSakvyxpK9QTdIbeDdwY426dwVm\nlnmVrYBZtq+W9HNglqRpwAPAsQC2F0qaBdxFFbROKQtaApwMXAiMBK4tG1S9qIslLQIepXp6LCIi\nBklfE/X/3rT/qYZ0v70B23cAB7bIXwkc1kuZGcCMFvnzgP1b5D8NHNNfWyIiYmD0GlRsv2EgGxIR\nEd2v30eKy7zICcD4xuOz9H1ERDSrs/T9NVTfH1lAjeVZIiJi6KoTVLax/Tcdb0lERHS9Oo8UXyzp\nJEm7lsUgd5K0U8dbFhERXadOT+V/gM8Cn2D9U18my99HRESTOkHlb4G9bT/S75ERETGk1Rn+WgQ8\n1emGRERE96vTU3kSuE3Sj6iWvwfySHFERPyhOkHle2WLiIjoU533qczs75iIiAio9436xbRY68t2\nnv6KiIgN1Bn+mtSQ3oZqAcd8TyUiIv5Av09/2V7ZsC2z/XngrQPQtoiI6DJ1hr9e07C7FVXPpU4P\nJyIihpg6waHxvSprgfspL9aKiIhoVOfpr7xXJSIiaqkz/DUCeBd/+D6VT3euWRER0Y3qDH9dBawG\n5tPwjfqIiIhmdYLKONuTN7ZiSbsBFwFjqL7ncoHtL5Rl8y+j6vncDxxre1UpcwYwDVgHnGr7upI/\nEbgQGEn10rDTbLv0oi4CJgIrgXfbvn9j2xoREe1RZ0HJmyW98nnUvRb4W9v7AgcDp0jaFzgdmGN7\nAjCn7FM+mwLsB0wGzpM0rNR1PnASMKFsPUFuGrDK9t7AOcDZz6OdERHRJnWCyiHAfEn3SLpD0gJJ\nd/RXyPZy278s6ceBu4GxwJFAz9IvM4GjSvpI4FLbz9heTLU68kGSdgW2tz3Xtql6Jo1leuq6HDhM\nkmpcU0REdECd4a8jNvUkksYDBwK3AGNsLy8fPUg1PAZVwJnbUGxpyXu2pJvze8osAbC9VtJqYGdg\ng3e/SJoOTAfYfffdN/VyIiKiF3UeKX5gU04gaTvgu8BHbK9p7EiUeZE/WFes3WxfAFwAMGnSpI6f\nLyJiqKoz/PW8SdqaKqBcYvuKkv1QGdKi/Hy45C8DdmsoPq7kLSvp5vwNykgaDoyimrCPiIhB0LGg\nUuY2vg7cbftzDR/NBqaW9FSqR5Z78qdIGiFpT6oJ+VvLUNkaSQeXOk9oKtNT19HADWXeJSIiBkEn\n1/D6Y+C9wAJJt5W8jwNnAbMkTQMeoCz5YnuhpFnAXVRPjp1ie10pdzLrHym+tmxQBa2LJS0CHqV6\neiwiIgZJx4KK7Z8CvT2JdVgvZWYAM1rkzwP2b5H/NNVS/BERsRnIasMR0TavnNn6K20Lpi4Y4JbE\nYElQic1WfkFFdJ8ElRgQvQWIdtaVYBMx+BJUoq3aGTwiovskqMQWIz2YiMHX0S8/RkTE0JKgEhER\nbZPhr3heumnuJMNiEQMnPZWIiGibBJWIiGibDH/FkJVhsYj2S08lIiLaJj2V6FM3TchHxOBLTyUi\nItomPZWImjIHE9G/BJWIJoM15Pd8zpuAFpubDH9FRETbJKhERETbdGz4S9I3gLcBD9vev+TtBFwG\njAfuB461vap8dgYwDVgHnGr7upI/kfXvp78GOM22JY0ALgImAiuBd9u+v1PXE9EueaIutmSdnFO5\nEPgi1S/+HqcDc2yfJen0sv8xSfsCU4D9gJcAP5T0MtvrgPOBk4BbqILKZOBaqgC0yvbekqYAZwPv\n7uD1bNHyi+75ywR+xHodG/6yfRPwaFP2kcDMkp4JHNWQf6ntZ2wvBhYBB0naFdje9lzbpgpQR7Wo\n63LgMEnqzNVEREQdAz2nMsb28pJ+EBhT0mOBJQ3HLS15Y0u6OX+DMrbXAquBnVudVNJ0SfMkzVux\nYkU7riMiIloYtIn60vPwAJ3rAtuTbE8aPXr0QJwyImJIGujvqTwkaVfby8vQ1sMlfxmwW8Nx40re\nspJuzm8ss1TScGAU1YR9xGYh81QxFA10T2U2MLWkpwJXNeRPkTRC0p7ABODWMlS2RtLBZb7khKYy\nPXUdDdxQej8RETFIOvlI8beBQ4FdJC0FPgWcBcySNA14ADgWwPZCSbOAu4C1wCnlyS+Ak1n/SPG1\nZQP4OnCxpEVUDwRM6dS1RMSG0guL3nQsqNg+rpePDuvl+BnAjBb584D9W+Q/DRyzKW2MiIj2ytpf\nQ0z+woyITsoyLRER0TbpqUREr9KzjY2VnkpERLRNgkpERLRNhr8iIsNc0TYJKhFDSIJHdFqCyhYq\nvzwiYjBkTiUiItomPZWILVB6qjFYElQiuli3BI+8HXPoyPBXRES0TYJKRES0TYJKRES0TeZUuly3\njKlHxNCQnkpERLRNgkpERLRNgkpERLRN5lS6ROZOIqIbdH1PRdJkSfdIWiTp9MFuT0TEUNbVQUXS\nMOBLwBHAvsBxkvYd3FZFRAxd3T78dRCwyPZ9AJIuBY4E7hrUVtWQ4ayI2BJ1e1AZCyxp2F8KvK75\nIEnTgell9wlJ9wxA2wbaLsAjg92IzUTuRWWzvw96nwbiNJv9fRgoep825V7sUeegbg8qtdi+ALhg\nsNvRSZLm2Z402O3YHOReVHIfKrkP6w3EvejqORVgGbBbw/64khcREYOg24PKL4AJkvaU9AJgCjB7\nkNsUETFkdfXwl+21kv4KuA4YBnzD9sJBbtZg2aKH9zZS7kUl96GS+7Bex++FbHf6HBERMUR0+/BX\nRERsRhJUIiKibRJUukx/y9JIOl7SHZIWSLpZ0qsHo52dVnd5HkmvlbRW0tED2b6BVOdeSDpU0m2S\nFkr68UC3cSDU+H9jlKTvS7q93IcTB6OdnSbpG5IelnRnL59L0rnlPt0h6TVtbYDtbF2yUT2M8Bvg\npcALgNuBfZuO+SNgx5I+ArhlsNs9GPeh4bgbgGuAowe73YP4b2IHqlUmdi/7Lx7sdg/Sffg4cHZJ\njwYeBV4w2G3vwL34U+A1wJ29fP4W4FpAwMHt/h2Rnkp3+f2yNLb/B+hZlub3bN9se1XZnUv13Z0t\nTb/3ofgw8F3g4YFs3ACrcy/+ArjC9m8BbG+J96POfTDwIkkCtqMKKmsHtpmdZ/smqmvrzZHARa7M\nBXaQtGu7zp+g0l1aLUszto/jp1H9RbKl6fc+SBoL/Dlw/gC2azDU+TfxMmBHSTdKmi/phAFr3cCp\ncx++CLwC+B2wADjN9nMD07zNysb+HtkoXf09leidpDdQBZVDBrstg+TzwMdsP1f9YTqkDQcmAocB\nI4GfS5pr+78Ht1kD7nDgNuCNwF7A9ZJ+YnvN4DZry5Kg0l1qLUsj6VXA14AjbK8coLYNpDr3YRJw\naQkouwBvkbTW9vcGpokDps69WAqstP0k8KSkm4BXA1tSUKlzH04EznI1sbBI0mJgH+DWgWniZqOj\ny1tl+Ku79LssjaTdgSuA927Bf4n2ex9s72l7vO3xwOXAyVtgQIF6SxVdBRwiabikF1Kt5H33ALez\n0+rch9/bVIPAAAAE00lEQVRS9daQNAZ4OXDfgLZy8zAbOKE8BXYwsNr28nZVnp5KF3Evy9JI+mD5\n/MvAPwI7A+eVv9LXegtbobXmfRgS6twL23dL+gFwB/Ac8DXbLR837VY1/038M3ChpAVUTz59zPYW\ntyS+pG8DhwK7SFoKfArYGn5/H66hegJsEfAUVQ+ufecvj5hFRERssgx/RURE2ySoRERE2ySoRERE\n2ySoRERE2ySoRERE2ySoxGZP0g6STm7K+2xZafazLY6/RtIOLfLPlPTRkt6nrNr7K0l7bWL77pe0\ny6bU0U/9IyT9sLT33R06x8c7UW8MPQkq0Q12AE5uypsOvMr23zUfbPstth/rp86jgMttH2j7N21q\nZ6ccCGD7ANuXdegcCSrRFgkq0Q3OAvYqf6l/VtJsqlVm57f6y72x5yDpE5L+W9JPqb5BjaS3AB8B\nPiTpR01lP9jY+5H0PklfLOnvlQUZF0qa3uK84xvfYSHpo5LOLOm9JP2glP+JpH1alN+pnOMOSXMl\nvUrSi4FvAa8t179XU5lTJd1Vylxa8rYt79S4tfTEjmy4litKO+6V9K8l/yxgZKn/kpL3nlL+Nklf\nkTSs5D8haYaqd5LMLd9MR9IYSVeW/Nsl/VFf9cQWbLDX/s+Wrb8NGE/TuyGAJ/o4/n6q9b4mUq1G\n+0Jge6pvEH+0HHNmT7qp7GiqJdR79q8FDinpncrPkcCdwM5N59ugncBHgTNLeg4woaRfB9zQ4tz/\nAXyqpN8I3FbShwJX93KtvwNGlPQO5edngPf05FGt8bUt8D6qZUlGAdsADwC7Nd9PqpV8vw9sXfbP\nA04oaQNvL+l/BT5Z0pcBHynpYeUcvdaTbcvdskxLbMn+BLjS9lMApYfTJ9srJN1X1kS6l2rBwZ+V\nj0+V9OclvRswAeh3wU5J21G9PO07Wr9i8ogWhx4CvKu04wZJO0vavp/q7wAukfQ9oGdtsz8D3tEz\nf0QVQHYv6Tm2V5d23QXswYbLoEO1PtZE4BelvSNZ/06a/wGuLun5wJtL+o3ACaXt64DVkt7bRz2x\nhUpQia5WhlPml93Ztv+xDdVeChwL/JoqKFnSocCbgNfbfkrSjVS/rButZcMh5Z7PtwIes31AG9rW\n7K1Ub/p7O/AJSa+kWtfqXbbvaTxQ0uuAZxqy1tH6d4CAmbbPaPHZs7Z71nbqrXydemILlTmV6AaP\nAy9q9YHtda4msA9oEVBuAo6SNFLSi6h+8dZxJdXb8Y6jCjBQDeesKgFlH6rXsDZ7CHhx6WGMAN5W\n2rgGWCzpGPj9O8Jf3aL8T4DjyzGHAo+4j3d9SNqKavjqR8DHShu3o1pU8cMq3QNJB9a45mclbV3S\nc4Cjy3xOz1zPHv2UnwN8qBw/TNKo51lPdLkEldjsuXonzM8k3akWjxD3Ue6XVGP9t1PNjfyiZrlV\nVEvD72G7510bPwCGS7qb6sGBuS3KPQt8mur9HNdT9XR6HA9Mk3Q7sJDWrz8+E5go6Y5yjqn9NHUY\n8C1Vq+7+CjjX1VNv/0y1Ku0dkhaW/f5cUI6/xPZdwCeB/1vacj3Q3+tmTwPeUNoyn+r98M+nnuhy\nWaU4IiLaJj2ViIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIhomwSViIhom/8PBsrY0OK+\n0WUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182498048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the tf-idf distribution over sentences\n",
    "\n",
    "\n",
    "# the histogram of the data\n",
    "ax = plt.hist([x for x in stfidf if str(x) != 'nan'], 50)\n",
    "\n",
    "plt.xlabel('tf-idf value of sentence')\n",
    "plt.ylabel('number of sentences')\n",
    "plt.title('tf-idf dist over sentences')\n",
    "#labels = [item.get_text() for item in ax.get_yticklabels()]\n",
    "#ax.set_xticklabels(labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add a topic model. Our goal is to pare down the tf-idf space into a subspace defined by major topic axes, and use topics as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)]\n",
      "[(2, 0.1217169239582691), (3, 0.30230312910390755), (4, 0.1828764398150676), (5, 0.017012819573171706), (6, 0.2557849334233673), (7, 0.8665596984748893), (8, 0.05368257118654773), (9, 0.019686602255689127), (10, 0.20110583368465484)]\n",
      "[(0, 0.21326306826923844), (1, 0.050749118766525977), (2, 0.011672720479926013), (3, 0.0018034765397566366), (4, -0.017344280899082361), (5, -0.027501439065333137), (6, -0.011674899593848299), (7, -0.00071573178318109706), (8, 0.04267500172128022), (9, 0.048574825174033123), (10, 0.026575998528348774), (11, 0.044421532996683409), (12, 0.013393446951918003), (13, -0.030710753112995336), (14, -0.0042430543714664162), (15, 0.026780671636093815), (16, -0.048507526811905066), (17, -0.018350852246359284), (18, -0.0088498523285759148), (19, -0.0036967389008031003), (20, -0.00033181989194429155), (21, -0.048398855798007866), (22, 0.010960048997954872), (23, 0.0013603064206361322), (24, 0.0064444425012393745), (25, 0.012269113273438357), (26, -0.025584454071573802), (27, 0.002979323396870643), (28, -0.018492097995549048), (29, -0.012231450495358863), (30, 0.009924344789301728), (31, 0.0040389067812181476), (32, 0.08265492060074027), (33, 0.025565350590875658), (34, -0.045587211462218488), (35, 0.040885586184833379), (36, 0.024052891202645543), (37, 0.014487922845404417), (38, -0.027008363830642385), (39, 0.0064207420130769962), (40, -0.0035378643668928949), (41, -0.015760438512768829), (42, 0.025198085353735668), (43, 0.018897419888105161), (44, 0.048056013852989687), (45, 0.0086660259890860569), (46, -0.015173581984870239), (47, 0.026643027279604804), (48, -0.015900829340503016), (49, 0.01484272149501447), (50, -0.018386986256640543), (51, 0.03708405919297493), (52, -0.017954521233918677), (53, -0.024137866405737004), (54, -0.019675840366069827), (55, 0.022335822140201508), (56, 0.059722908288135312), (57, 0.019808093967113496), (58, 0.0073873818149243885), (59, 0.01313894144072141), (60, 0.040736602138248298), (61, -0.012003811125019442), (62, -0.0013874436965890151), (63, 0.0020072171777286468), (64, -0.010673621872829307), (65, -0.016747854403937751), (66, 0.0020799139651584762), (67, -0.02617781118490279), (68, 0.011881957243332075), (69, 0.026494520103958644), (70, 0.034533222007427158), (71, -0.040136201985763301), (72, 0.016338205283767834), (73, 0.0057954838354784578), (74, 0.0059964746071086324), (75, 0.0013773432099199838), (76, -0.036993821755857324), (77, 0.0043949512340054888), (78, 0.047140484928212788), (79, 0.0062958787878601118), (80, 0.013791396784112755), (81, 0.040649293084790851), (82, 0.046230165924175726), (83, 0.05275450820041884), (84, -0.0088094490628420358), (85, 0.021883329907651274), (86, -0.051412599518523422), (87, -0.068227382592334782), (88, -0.00201188672107264), (89, -0.031640247019558476), (90, 0.0031328515120854971), (91, 0.047975049915455203), (92, 0.0076709757438762158), (93, -0.0033132076956915563), (94, 0.01834650383385342), (95, -0.0092355464794194771), (96, -0.012381726649098894), (97, 0.033400069753809075), (98, -0.039899140674564092), (99, 0.0036934752126211098), (100, -0.008595243168143727), (101, -0.023399852491865016), (102, 9.1206557946104175e-05), (103, -0.033918943959628182), (104, 0.015708161016900861), (105, -0.02136344347149775), (106, -0.0041222283748293832), (107, -0.020222412260916513), (108, -0.0135850036898237), (109, -0.039423825781294192), (110, 0.031365600159666399), (111, 0.02514374542451412), (112, -0.032527793314102355), (113, 0.039370457818309515), (114, 0.001366891618248281), (115, 0.084853089778486088), (116, 0.0032446417353609539), (117, 0.030040982289076101), (118, -0.022982825942570291), (119, 0.0012050285098965489), (120, 0.0099487855118012142), (121, -0.046079475722594659), (122, 0.011862285488163272), (123, -0.0046310014398989923), (124, 0.0064228538513656144), (125, 0.0053727280730259926), (126, 0.0049130544078688897), (127, 0.056850556833942717), (128, 0.017974451049561649), (129, 0.0024144334468734555), (130, -0.0082173590622448122), (131, -0.021249951263237025), (132, -0.034288534845728622), (133, 0.017511168515558017), (134, -0.031156494527595571), (135, 0.029368401205300907), (136, 0.043088907854408214), (137, -0.0056364081963821526), (138, 0.071568844906326479), (139, -0.068679805429075036), (140, 0.0071164798515020496), (141, 0.021409006186402561), (142, 0.020183139929118694), (143, 0.030510490569758974), (144, 0.031102939411911765), (145, 0.042330720430220695), (146, 0.0012490038121745323), (147, 0.0076129811176851338), (148, 0.018049919788327734), (149, 0.007582657475344215), (150, -0.044865614112630979), (151, -0.058987757472800391), (152, 0.0019562402213283564), (153, 0.007995756247998977), (154, 0.014600839843889081), (155, -0.020225744412095067), (156, 0.046975539982746999), (157, -0.034981939890538299), (158, -0.02383465687112608), (159, 0.045075324023969965), (160, -0.008143242389756734), (161, -0.034314175600668427), (162, 0.01119469561343849), (163, -0.068918401345135299), (164, -0.024785054376130745), (165, 0.016936741889141745), (166, -0.015962000071452043), (167, -0.014586121830249311), (168, 0.00077695461999283903), (169, 0.033028769518002689), (170, -0.016265790550611347), (171, -0.0046346950203727533), (172, -0.036053592586266087), (173, 0.026965223202597408), (174, 0.024126251081134191), (175, -0.0053802039303286623), (176, 0.0224226829170459), (177, 0.021269967052491193), (178, -0.0076975799335320888), (179, 0.012318830510350889), (180, 0.0076356878183412736), (181, 0.032001209537293511), (182, -0.0025999689013015744), (183, -0.034814752386880976), (184, 0.024539648869314208), (185, 0.028298506079344792), (186, 0.025625411025715049), (187, -0.047069806716777504), (188, 0.045549297440010059), (189, 0.049269089796656285), (190, 0.018055646684573649), (191, -0.00066155841116207707), (192, 0.021920418994578736), (193, -0.091447558432031548), (194, -0.0016201127759123387), (195, -0.032306425359782454), (196, 0.00058493670219325351), (197, -0.0011864056984553497), (198, 0.0067878351375102088), (199, -0.050717468409968582)]\n"
     ]
    }
   ],
   "source": [
    "# create LSI model (try LDA later?)\n",
    "lsi = models.LsiModel(corp_tfidf, id2word=dictionary, num_topics=200)\n",
    "corp_lsi = lsi[corp_tfidf]\n",
    "sentence_lsi = lsi[sentence_corp]\n",
    "print(new_vec)          # in original dictionary\n",
    "print(tfidf[new_vec])   # in tf-idf space\n",
    "print(lsi[new_vec])     # in LSI/LSA topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.098*\"design\" + 0.082*\"life\" + 0.079*\"book\" + 0.077*\"user\" + 0.075*\"product\" + 0.074*\"compani\" + 0.070*\"learn\" + 0.068*\"success\" + 0.066*\"app\" + 0.065*\"busi\"'),\n",
       " (1,\n",
       "  '-0.322*\"user\" + -0.306*\"design\" + 0.237*\"trump\" + -0.219*\"app\" + -0.138*\"ux\" + -0.114*\"code\" + -0.104*\"ui\" + 0.101*\"women\" + 0.096*\"presid\" + -0.095*\"product\"'),\n",
       " (2,\n",
       "  '0.407*\"trump\" + 0.165*\"user\" + 0.150*\"presid\" + 0.150*\"clinton\" + 0.143*\"vote\" + 0.134*\"elect\" + 0.132*\"hillari\" + 0.117*\"design\" + -0.111*\"book\" + 0.109*\"women\"'),\n",
       " (3,\n",
       "  '-0.575*\"que\" + -0.395*\"de\" + -0.253*\"não\" + -0.230*\"é\" + -0.217*\"e\" + -0.149*\"da\" + -0.148*\"um\" + -0.143*\"uma\" + -0.140*\"para\" + -0.121*\"se\"'),\n",
       " (4,\n",
       "  '0.275*\"creatom\" + 0.238*\"jon\" + 0.177*\"startup\" + 0.174*\"compani\" + 0.168*\"agenc\" + -0.138*\"habit\" + 0.127*\"market\" + 0.127*\"gl\" + 0.126*\"goo\" + 0.123*\"westenberg\"'),\n",
       " (5,\n",
       "  '0.384*\"trump\" + -0.264*\"women\" + -0.174*\"men\" + -0.162*\"black\" + 0.113*\"clinton\" + -0.111*\"woman\" + -0.110*\"girl\" + -0.104*\"white\" + 0.100*\"presid\" + 0.092*\"success\"'),\n",
       " (6,\n",
       "  '-0.325*\"code\" + 0.276*\"design\" + 0.266*\"user\" + -0.238*\"javascript\" + 0.167*\"ux\" + 0.156*\"creatom\" + -0.142*\"develop\" + -0.139*\"program\" + -0.138*\"react\" + 0.133*\"jon\"'),\n",
       " (7,\n",
       "  '0.300*\"basecamp\" + -0.226*\"creatom\" + -0.209*\"code\" + 0.208*\"compani\" + -0.191*\"javascript\" + -0.176*\"jon\" + -0.149*\"react\" + -0.126*\"agenc\" + 0.121*\"employe\" + 0.118*\"custom\"'),\n",
       " (8,\n",
       "  '-0.390*\"design\" + 0.333*\"app\" + -0.240*\"basecamp\" + 0.171*\"appl\" + 0.162*\"user\" + -0.142*\"code\" + -0.130*\"sketch\" + 0.127*\"snapchat\" + -0.118*\"color\" + 0.113*\"facebook\"'),\n",
       " (9,\n",
       "  '0.440*\"basecamp\" + -0.233*\"women\" + 0.187*\"book\" + 0.145*\"trump\" + -0.124*\"men\" + -0.113*\"user\" + 0.110*\"write\" + 0.106*\"read\" + 0.102*\"rework\" + 0.100*\"1999\"'),\n",
       " (10,\n",
       "  '-0.391*\"book\" + 0.314*\"basecamp\" + -0.190*\"read\" + 0.185*\"app\" + -0.147*\"design\" + -0.138*\"medium\" + -0.126*\"write\" + -0.113*\"writer\" + 0.106*\"user\" + 0.105*\"react\"'),\n",
       " (11,\n",
       "  '-0.465*\"women\" + -0.242*\"basecamp\" + -0.239*\"men\" + -0.133*\"task\" + -0.129*\"habit\" + -0.126*\"woman\" + -0.125*\"app\" + 0.105*\"lebron\" + -0.102*\"book\" + -0.094*\"male\"'),\n",
       " (12,\n",
       "  '-0.280*\"user\" + -0.268*\"basecamp\" + 0.200*\"sketch\" + 0.191*\"design\" + -0.153*\"book\" + 0.145*\"task\" + 0.138*\"morn\" + 0.129*\"sleep\" + 0.128*\"team\" + 0.117*\"habit\"'),\n",
       " (13,\n",
       "  '-0.398*\"black\" + -0.250*\"white\" + -0.237*\"color\" + 0.217*\"user\" + 0.182*\"women\" + 0.159*\"trump\" + -0.151*\"polic\" + -0.143*\"sketch\" + -0.131*\"basecamp\" + -0.124*\"postanli\"'),\n",
       " (14,\n",
       "  '0.260*\"book\" + -0.193*\"app\" + -0.178*\"postanli\" + 0.131*\"creatom\" + 0.119*\"read\" + 0.119*\"black\" + -0.116*\"trump\" + 0.100*\"sleep\" + -0.098*\"sketch\" + 0.098*\"basecamp\"'),\n",
       " (15,\n",
       "  '-0.264*\"sketch\" + -0.235*\"compon\" + 0.226*\"code\" + -0.202*\"react\" + 0.199*\"black\" + 0.160*\"user\" + -0.157*\"women\" + 0.118*\"program\" + 0.113*\"white\" + 0.112*\"learn\"'),\n",
       " (16,\n",
       "  '-0.340*\"book\" + -0.332*\"appl\" + 0.254*\"medium\" + -0.224*\"app\" + 0.139*\"content\" + 0.131*\"media\" + -0.121*\"iphon\" + -0.120*\"women\" + 0.117*\"snapchat\" + 0.115*\"facebook\"'),\n",
       " (17,\n",
       "  '-0.286*\"lebron\" + -0.200*\"durant\" + -0.187*\"nba\" + 0.177*\"appl\" + -0.175*\"black\" + -0.168*\"team\" + -0.163*\"game\" + -0.151*\"player\" + -0.148*\"user\" + 0.132*\"sketch\"'),\n",
       " (18,\n",
       "  '0.283*\"black\" + -0.196*\"lebron\" + -0.175*\"women\" + 0.173*\"white\" + -0.134*\"durant\" + -0.132*\"game\" + -0.131*\"basecamp\" + 0.129*\"react\" + -0.128*\"medium\" + 0.127*\"slack\"'),\n",
       " (19,\n",
       "  '-0.279*\"slack\" + 0.226*\"color\" + -0.171*\"bot\" + -0.161*\"team\" + 0.127*\"money\" + -0.125*\"design\" + 0.121*\"postanli\" + -0.119*\"app\" + 0.106*\"wordstream\" + 0.105*\"inc\"')]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the most important topics in LSI\n",
    "lsi.print_topics(20)\n",
    "#[print(doc) for ndoc,doc in enumerate(corpus_lsi) if ndoc<20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get each sentence's main topic. Do they cluster by article or not?\n",
    "slsi = []\n",
    "for sent_rep in sentence_lsi:\n",
    "    #slsi = mean([wr[1] for wr in sent_rep])  # this needs to pick out a main topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HERE will be a plot that shows whether the sentence topics cluster by article.\n",
    "# 2D space. Use 10 articles to start. Different color for article.\n",
    "Make the scatter plot.....\n",
    "\n",
    "\n",
    "# OR try 1D space and have D2 be the article ID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now, we set up for similarity queries \n",
    "sim_index = similarities.Similarity('.',lsi[corp],num_features = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['today', 'air', 'forc', 'one', 'touch', 'havana', 'first', 'time', 'histori']\n",
      "[ 0.07958278 -0.03962181  0.16909875 ..., -0.15677771 -0.00896206\n",
      " -0.21422096]\n",
      "[ 0.07958278 -0.03962181  0.16909875 ..., -0.15677771 -0.00896206\n",
      " -0.21422096]\n"
     ]
    }
   ],
   "source": [
    "# find similarity between one highlight and each article\n",
    "h_sim = sim_index[new_vec,new_vec]\n",
    "#sorted_sims = sorted(enumerate(h_sim), key=lambda item: -item[1])\n",
    "sorted_sims[0:10]\n",
    "print(test_doc)\n",
    "[ttext[ix] for ix,csim in sorted_sims[1:10]]\n",
    "print(h_sim[0])\n",
    "print(h_sim[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'gensim.models.lsimodel.LsiModel'>\n"
     ]
    }
   ],
   "source": [
    "# find cosine similarity of each sentence to its own article\n",
    "dfS.head()\n",
    "\n",
    "print(type(lsi))\n",
    "\n",
    "# sent_own_sims = []\n",
    "# for ax in range(len(ptext)):\n",
    "#     if ax==0 or ax==1:#ax!=637:\n",
    "#         s_sims = sim_sent_own_doc(ptext[ax],sim_index,ax,dictionary,lsi)\n",
    "#         sent_own_sims.extend( s_sims )\n",
    "\n",
    "[sent_own_sims.extend(sim_sent_own_doc(ptext[ax],sim_index,ax,dictionary,lsi)) \n",
    "for ax in range(len(ptext)) if ax!=637]        \n",
    "        \n",
    "#dfS[['postid','sentence']]\n",
    "print(sent_own_sims)\n",
    "#print(ptext[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sim_sent_own_doc(docsents,index,docpos,dicty,mod):\n",
    "    '''finds similarity of each sentence to its own doc'''\n",
    "    a_lsi = [mod[dicty.doc2bow(s)] for s in docsents]\n",
    "    sims = index[a_lsi]\n",
    "    sims = [s[docpos] for s in sims]\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine similarity of sentence to title\n",
    "print(len(sent_own_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find sentiment of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find sentiment of each article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Balancing Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Downsample negative sentences so ratio is ~2:1 neg:pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Upsample positive sentence with data augmentation\n",
    "\n",
    "# ideas: do they cluster in tf-idf space? try jitter...\n",
    "#        shuffle/swap words from other highlights?\n",
    "#        redistribute among highlights in the same article..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
